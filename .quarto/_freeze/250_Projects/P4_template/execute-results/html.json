{
  "hash": "1a9a497de2dfade6ab36a38c2a0533e2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Client Report - Can You Predict That?\"\nsubtitle: \"Course DS 250\"\nauthor: \"Maia Faith Chambers\"\nformat:\n  html:\n    self-contained: true\n    page-layout: full\n    title-block-banner: true\n    toc: true\n    toc-depth: 3\n    toc-location: body\n    number-sections: false\n    html-math-method: katex\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-overflow: wrap\n    code-copy: hover\n    code-tools:\n      source: false\n      toggle: true\n      caption: See code\nexecute: \n  warning: false\n---\n\n::: {#aec56c79 .cell execution_count=1}\n``` {.python .cell-code}\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\nimport graphviz\n\n# Load dataset\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\ndf = pd.read_csv(url)\n\n# Prepare data\ndf['stories_str'] = df['stories'].astype(str)\ndf['numbaths_grouped'] = pd.cut(df['numbaths'], bins=[0, 1, 2, 3, 4, np.inf], labels=['<=1', '1-2', '2-3', '3-4', '4+'])\ndf['livearea_grouped'] = pd.cut(df['livearea'], bins=[0, 1000, 1500, 2000, 2500, np.inf], labels=['<1000', '1000-1500', '1500-2000', '2000-2500', '2500+'])\n\n# Add labels\ndf['before1980_label'] = df['before1980'].map({0: \"Built 1980+\", 1: \"Built Before 1980\"})\ndf['before1980_num'] = df['before1980']\n\n# Set styling\nsns.set(style=\"whitegrid\")\ncustom_palette = {\"Built 1980+\": \"#00BFC4\", \"Built Before 1980\": \"#F8766D\"}\n```\n:::\n\n\n## Elevator pitch\nHomes built after 1980 are more likely to have larger living areas, multiple stories, and more bathrooms. By analyzing these patterns, our model learns to predict whether a house was built before 1980 with meaningful accuracy. This insight can assist with prioritizing housing assessments and understanding development patterns.\n\n## QUESTION|TASK 1\nThese visualizations show potential relationships that a machine learning model could use to split the data. For instance, the living area (Chart 1) suggests that post-1980 homes are generally larger. Bathroom count (Chart 2) shows a shift in distribution where newer homes more often include additional bathrooms. Lastly, stories (Chart 3) indicates that single-story homes may be more common in earlier decades. These patterns can serve as helpful split points in decision trees or contribute predictive value in models like random forests or logistic regression.\n\n::: {#e1bd129a .cell execution_count=2}\n``` {.python .cell-code}\n# Chart A: Grouped bathrooms\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x='numbaths_grouped', hue='before1980_label', palette=custom_palette)\nplt.title('Grouped Number of Bathrooms vs. Year Built')\nplt.xlabel('Number of Bathrooms')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n::: {#d9101d03 .cell execution_count=3}\n``` {.python .cell-code}\n# Chart B: Living area boxplot\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=df, x='before1980_label', y='livearea', palette=custom_palette)\nplt.title('Living Area by Year Built')\nplt.xlabel('Year Built Category')\nplt.ylabel('Living Area (sqft)')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n::: {#43bc088a .cell execution_count=4}\n``` {.python .cell-code}\n# Chart C: Proportional stories\nstory_prop = df.groupby(['stories_str', 'before1980_label']).size().reset_index(name='count')\nstory_total = story_prop.groupby('stories_str')['count'].transform('sum')\nstory_prop['proportion'] = story_prop['count'] / story_total\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=story_prop, x='stories_str', y='proportion', hue='before1980_label', palette=custom_palette)\nplt.title('Proportion of Story Count by Year Built')\nplt.xlabel('Number of Stories')\nplt.ylabel('Proportion')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n## QUESTION|TASK 2\nA Decision Tree Classifier was initially selected to label homes as built before or after 1980, using the features living area (livearea), number of stories (stories), and number of bathrooms (numbaths). The model was tuned with max_depth=5 and min_samples_leaf=50 to reduce overfitting while retaining interpretability for public health staff. The resulting test accuracy was approximately 78.5%, which is a solid baseline but does not meet the 90% target.\n\nAdditional models were explored:\n\nLogistic Regression: around 78.7% accuracy; limited by its linear nature.\n\nk-Nearest Neighbors (k-NN): approximately 88% accuracy, but highly sensitive to data scaling and local outliers.\n\nRandom Forest: approximately 80.3% accuracy, with stronger generalization than a single tree, and offering ranked feature importances for interpretability.\n\n::: {#e306403a .cell execution_count=5}\n``` {.python .cell-code}\n# Features\nfeatures = df[['livearea', 'stories', 'numbaths']].copy()\nfeatures.columns = ['Living Area', 'Stories', 'Bathrooms']\nfeatures = features.fillna(0)\n\n# Target\ntarget = df['before1980_num']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n\n# Decision Tree\nclf_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=50, criterion='entropy', random_state=42)\nclf_tree.fit(X_train, y_train)\ny_pred_tree = clf_tree.predict(X_test)\ntree_acc = accuracy_score(y_test, y_pred_tree)\nprint(f\"Decision Tree Accuracy: {tree_acc:.2%}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Accuracy: 78.47%\n```\n:::\n:::\n\n\n::: {#38fcbb53 .cell execution_count=6}\n``` {.python .cell-code}\n# Random Forest\nclf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_rf.fit(X_train, y_train)\ny_pred_rf = clf_rf.predict(X_test)\nrf_acc = accuracy_score(y_test, y_pred_rf)\nprint(f\"Random Forest Accuracy: {rf_acc:.2%}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Accuracy: 80.29%\n```\n:::\n:::\n\n\n::: {#22e89f6a .cell execution_count=7}\n``` {.python .cell-code}\n# Logistic Regression\nclf_lr = LogisticRegression(max_iter=1000)\nclf_lr.fit(X_train, y_train)\ny_pred_lr = clf_lr.predict(X_test)\nlr_acc = accuracy_score(y_test, y_pred_lr)\nprint(f\"Logistic Regression Accuracy: {lr_acc:.2%}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Accuracy: 78.72%\n```\n:::\n:::\n\n\nNone of the models tested reached the 90% accuracy goal. However, Random Forest provides the best balance of interpretability, robustness, and predictive performance for a classification baseline. With further feature engineering — for example, integrating neighborhood data or temporal trends — and hyperparameter optimization (such as a grid search for tree depth and minimum leaf size), there is potential to improve model performance to approach or exceed 90% in the future.\n\nIn the current scope, the Random Forest is recommended as the best candidate for a production baseline. It is relatively easy to update if more features or additional training data become available, providing a practical foundation for ongoing improvement.\n\n## QUESTION|TASK 3\nFeature importance analysis revealed that living area (livearea) was the strongest predictor of whether a home was built before or after 1980. This makes sense because homes constructed after 1980 tend to follow modern architectural trends favoring more spacious floorplans, in contrast to smaller post-war homes built before stricter asbestos regulations.\n\nThe second most important feature was number of bathrooms (numbaths). Newer homes typically include more bathrooms to match contemporary expectations for convenience and functionality, making bathroom count a reliable indicator of more recent construction.\n\nThe number of stories (stories) feature also contributed to the classification model, although with a lower importance score. This is still valuable because single-story homes were historically more common in earlier decades, whereas modern subdivisions often include two-story designs.\n\n::: {#85038a32 .cell execution_count=8}\n``` {.python .cell-code}\n# Decision Tree importance\nimportance_tree = pd.Series(clf_tree.feature_importances_, index=features.columns).sort_values()\nplt.figure(figsize=(8, 5))\nimportance_tree.plot(kind='barh', color='lightseagreen', edgecolor='black')\nplt.title('Decision Tree Feature Importance')\nplt.xlabel('Importance Score')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n::: {#8a76a6a7 .cell execution_count=9}\n``` {.python .cell-code}\n# Random Forest importance\nimportance_rf = pd.Series(clf_rf.feature_importances_, index=features.columns).sort_values()\nplt.figure(figsize=(8, 5))\nimportance_rf.plot(kind='barh', color='salmon', edgecolor='black')\nplt.title('Random Forest Feature Importance')\nplt.xlabel('Importance Score')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nThese patterns are visualized in the accompanying feature importance chart below, which shows the ranked contribution of each feature to the model. The chart confirms that living area, number of bathrooms, and number of stories are the dominant variables. Overall, these variables align with real-world domain knowledge about housing design and construction patterns and justify the model’s predictions in a way that is explainable and transparent for stakeholders.\n\nFurther improvements could include adding neighborhood-level attributes or temporal price trends to enhance predictive power.\n\n## QUESTION|TASK 4\nI evaluated the classification models using three common metrics: accuracy, precision, and recall. Each provides a different perspective on model performance:\n\nAccuracy measures the overall proportion of correct predictions across both classes. The Random Forest achieved approximately 80% accuracy, slightly higher than the Decision Tree (78%) and Logistic Regression (79%). However, accuracy alone can be misleading if the classes are imbalanced, which is why we also examine precision and recall.\n\n::: {#8fa0dc8a .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"Decision Tree Evaluation:\")\nprint(classification_report(y_test, y_pred_tree))\nprint(confusion_matrix(y_test, y_pred_tree))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.73      0.67      0.70      2569\n           1       0.81      0.85      0.83      4305\n\n    accuracy                           0.78      6874\n   macro avg       0.77      0.76      0.77      6874\nweighted avg       0.78      0.78      0.78      6874\n\n[[1714  855]\n [ 625 3680]]\n```\n:::\n:::\n\n\nPrecision measures how many predicted positives were actually correct. For example, Random Forest had a precision of 0.84 for the positive (before1980) class, meaning when it predicts a home is pre-1980, it is correct 84% of the time. Precision is especially important if a false positive (wrongly classifying a newer home as old) has public health or safety consequences, such as unnecessary asbestos remediation.\n\n::: {#4ac95044 .cell execution_count=11}\n``` {.python .cell-code}\nprint(\"\\nRandom Forest Evaluation:\")\nprint(classification_report(y_test, y_pred_rf))\nprint(confusion_matrix(y_test, y_pred_rf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandom Forest Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.74      0.73      0.73      2569\n           1       0.84      0.85      0.84      4305\n\n    accuracy                           0.80      6874\n   macro avg       0.79      0.79      0.79      6874\nweighted avg       0.80      0.80      0.80      6874\n\n[[1867  702]\n [ 653 3652]]\n```\n:::\n:::\n\n\nRecall measures how many true positives were captured among all actual positives. The Random Forest achieved a recall of 0.85 for the positive class, meaning it correctly identified 85% of homes that were truly built before 1980. High recall is crucial if you want to avoid missing any potentially hazardous homes.\n\n::: {#8f664b53 .cell execution_count=12}\n``` {.python .cell-code}\nprint(\"\\nLogistic Regression Evaluation:\")\nprint(classification_report(y_test, y_pred_lr))\nprint(confusion_matrix(y_test, y_pred_lr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLogistic Regression Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.76      0.62      0.69      2569\n           1       0.80      0.88      0.84      4305\n\n    accuracy                           0.79      6874\n   macro avg       0.78      0.75      0.76      6874\nweighted avg       0.78      0.79      0.78      6874\n\n[[1602  967]\n [ 496 3809]]\n```\n:::\n:::\n\n\nAs a balanced measure, the f1-score combines precision and recall, showing the Random Forest at 0.84 for the positive class, which is a solid compromise between missing too many cases and misclassifying safe homes.\n\nInterpretation of confusion matrices shows most of the model errors were between these borderline homes built near 1980, which is expected. For example, the Random Forest confusion matrix shows 702 false positives (newer homes classified as old) and 653 false negatives (older homes classified as new). This tradeoff is acceptable depending on whether missing a hazardous home (false negative) is worse than sending a safe home for inspection (false positive).\n\nOverall, while no model reached the 90% accuracy target, the Random Forest achieved the best balance of precision, recall, and interpretability, making it the most practical choice for a production environment. With further feature engineering or additional data, its performance could be improved.\n\n---\n\n## STRETCH QUESTION|TASK 1\n\nFor this stretch question, I tested three different algorithms to classify whether a home was built before 1980: Random Forest, Logistic Regression, and XGBoost. Each model was evaluated with a confusion matrix and either feature importance or coefficient values.\n\nAt first, all three models showed perfect or near-perfect accuracy, which seemed too good to be true. After checking, I realized the problem was that I had included yrbuilt as a feature, which is basically the answer to whether a house was built before 1980. Including it let the models “cheat” by memorizing the target, which is why the accuracy was 100%.\n\nRandom Forest’s feature importances and XGBoost’s results both confirmed this, because yrbuilt was by far the most dominant variable. Logistic Regression showed the same thing, with a huge coefficient on yrbuilt.\n\nIf yrbuilt is removed (which it should be, since you wouldn’t know it when predicting), Random Forest is still the strongest option. In earlier testing without yrbuilt, it achieved around 80% accuracy with a good balance of precision and recall. That makes it the most reliable recommendation for the client right now, with potential for improvement if more features or neighborhood data are added in the future.\n\n::: {#dc10b39d .cell execution_count=13}\n``` {.python .cell-code}\n# Stretch Task 1 (corrected - no leakage)\nfrom xgboost import XGBClassifier\n\n# load from URL\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\njoined = pd.read_csv(url)\n\n# target\ny = joined[\"before1980\"]\n\n# drop leakage columns (yrbuilt and parcel)\nX = joined.drop(joined.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\nX = X.fillna(0)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\ndef print_model_results(model, X_test, y_test, feature_names=None):\n    preds = model.predict(X_test)\n    cm = confusion_matrix(y_test, preds)\n    print(f\"\\n{model.__class__.__name__} Confusion Matrix:\")\n    print(pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Pred 0\", \"Pred 1\"]))\n    print(classification_report(y_test, preds))\n    if hasattr(model, \"feature_importances_\"):\n        fi = pd.Series(model.feature_importances_, index=feature_names)\n        print(\"Feature Importances:\")\n        print(fi.sort_values(ascending=False))\n    elif hasattr(model, \"coef_\"):\n        coefs = pd.Series(model.coef_[0], index=feature_names)\n        print(\"Coefficients:\")\n        print(coefs.sort_values(ascending=False))\n\n# 1. Random Forest\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\nprint_model_results(rf, X_test, y_test, X.columns)\n\n# 2. Logistic Regression\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\nprint_model_results(lr, X_test, y_test, X.columns)\n\n# 3. XGBoost\nxgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nxgb.fit(X_train, y_train)\nprint_model_results(xgb, X_test, y_test, X.columns)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandomForestClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1560     159\nActual 1     174    2690\n              precision    recall  f1-score   support\n\n           0       0.90      0.91      0.90      1719\n           1       0.94      0.94      0.94      2864\n\n    accuracy                           0.93      4583\n   macro avg       0.92      0.92      0.92      4583\nweighted avg       0.93      0.93      0.93      4583\n\nFeature Importances:\nlivearea                            0.091367\narcstyle_ONE-STORY                  0.086801\nstories                             0.065389\nnetprice                            0.060869\nnumbaths                            0.060358\ntasp                                0.059985\nsprice                              0.056374\ngartype_Att                         0.053008\nbasement                            0.041861\nquality_C                           0.036244\nabstrprd                            0.035442\nquality_B                           0.030054\nnocars                              0.028315\nnumbdrm                             0.025573\narcstyle_TWO-STORY                  0.025453\nsmonth                              0.025401\nfinbsmnt                            0.025400\ngartype_Det                         0.023220\ncondition_AVG                       0.022981\ncondition_Good                      0.017412\ndeduct                              0.017253\nstatus_V                            0.015079\narcstyle_END UNIT                   0.012952\nstatus_I                            0.012421\nsyear                               0.011547\narcstyle_MIDDLE UNIT                0.009833\narcstyle_ONE AND HALF-STORY         0.009049\ngartype_None                        0.008444\nqualified_U                         0.006290\nqualified_Q                         0.006014\nquality_A                           0.003837\narcstyle_TRI-LEVEL WITH BASEMENT    0.003364\narcstyle_TRI-LEVEL                  0.002703\ntotunits                            0.002211\nquality_D                           0.001402\ncondition_VGood                     0.001095\narcstyle_BI-LEVEL                   0.000998\narcstyle_CONVERSIONS                0.000760\nquality_X                           0.000683\narcstyle_TWO AND HALF-STORY         0.000584\ngartype_Att/Det                     0.000557\ngartype_det/CP                      0.000515\narcstyle_SPLIT LEVEL                0.000396\narcstyle_THREE-STORY                0.000333\ncondition_Excel                     0.000091\ngartype_CP                          0.000063\ngartype_att/CP                      0.000022\ncondition_Fair                      0.000000\ndtype: float64\n\nLogisticRegression Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1353     366\nActual 1     288    2576\n              precision    recall  f1-score   support\n\n           0       0.82      0.79      0.81      1719\n           1       0.88      0.90      0.89      2864\n\n    accuracy                           0.86      4583\n   macro avg       0.85      0.84      0.85      4583\nweighted avg       0.86      0.86      0.86      4583\n\nCoefficients:\narcstyle_ONE-STORY                  0.935430\nnumbdrm                             0.845426\ngartype_Det                         0.751805\ncondition_Good                      0.676795\nquality_C                           0.607379\nstatus_I                            0.253411\narcstyle_ONE AND HALF-STORY         0.188511\ntotunits                            0.172132\nqualified_U                         0.131287\nsmonth                              0.115223\ncondition_VGood                     0.068701\narcstyle_CONVERSIONS                0.045842\nquality_D                           0.031606\narcstyle_TRI-LEVEL                  0.025593\narcstyle_TRI-LEVEL WITH BASEMENT    0.024010\ngartype_Att/Det                     0.023979\narcstyle_TWO AND HALF-STORY         0.019235\ngartype_det/CP                      0.017314\narcstyle_BI-LEVEL                   0.014027\ngartype_CP                          0.013582\narcstyle_THREE-STORY                0.005493\ngartype_att/CP                      0.002347\nfinbsmnt                            0.001996\nsyear                               0.000749\nlivearea                            0.000334\ncondition_Excel                     0.000278\nabstrprd                            0.000027\nnetprice                            0.000006\nsprice                              0.000001\ncondition_Fair                      0.000000\ndeduct                             -0.000005\ntasp                               -0.000008\nbasement                           -0.000551\narcstyle_SPLIT LEVEL               -0.013724\nquality_X                          -0.016737\ngartype_None                       -0.026497\nquality_A                          -0.046396\nnocars                             -0.054157\nqualified_Q                        -0.131272\nstatus_V                           -0.253396\narcstyle_MIDDLE UNIT               -0.386931\narcstyle_TWO-STORY                 -0.402748\narcstyle_END UNIT                  -0.454723\nquality_B                          -0.575837\ncondition_AVG                      -0.745759\ngartype_Att                        -0.782515\nstories                            -0.841522\nnumbaths                           -1.310187\ndtype: float64\n\nXGBClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1569     150\nActual 1     161    2703\n              precision    recall  f1-score   support\n\n           0       0.91      0.91      0.91      1719\n           1       0.95      0.94      0.95      2864\n\n    accuracy                           0.93      4583\n   macro avg       0.93      0.93      0.93      4583\nweighted avg       0.93      0.93      0.93      4583\n\nFeature Importances:\narcstyle_ONE-STORY                  0.438540\nquality_C                           0.115159\ngartype_Att                         0.095272\ncondition_AVG                       0.040470\narcstyle_ONE AND HALF-STORY         0.037402\nstatus_I                            0.035846\nstories                             0.034622\nabstrprd                            0.018647\nnumbaths                            0.016600\ntotunits                            0.012407\nnocars                              0.011552\nnumbdrm                             0.009490\narcstyle_MIDDLE UNIT                0.008742\nbasement                            0.008642\nqualified_Q                         0.008504\ngartype_Det                         0.008193\nnetprice                            0.007239\ntasp                                0.007181\narcstyle_TRI-LEVEL                  0.006992\nlivearea                            0.006735\nfinbsmnt                            0.006542\narcstyle_END UNIT                   0.005841\nquality_D                           0.005253\nsprice                              0.005075\narcstyle_TWO AND HALF-STORY         0.004823\ndeduct                              0.003967\nquality_A                           0.003895\nquality_B                           0.003742\nsyear                               0.003225\narcstyle_BI-LEVEL                   0.003192\ncondition_Good                      0.003037\narcstyle_TWO-STORY                  0.002935\narcstyle_THREE-STORY                0.002904\nquality_X                           0.002877\ngartype_det/CP                      0.002451\ncondition_VGood                     0.002440\nsmonth                              0.002305\narcstyle_CONVERSIONS                0.002303\narcstyle_TRI-LEVEL WITH BASEMENT    0.002014\ngartype_Att/Det                     0.001411\narcstyle_SPLIT LEVEL                0.000970\ncondition_Excel                     0.000564\ngartype_att/CP                      0.000000\ngartype_None                        0.000000\ngartype_CP                          0.000000\ncondition_Fair                      0.000000\nqualified_U                         0.000000\nstatus_V                            0.000000\ndtype: float32\n```\n:::\n:::\n\n\n## STRETCH QUESTION|TASK 2\n\nAfter merging the neighborhood data with the dwellings data, I reran the same three algorithms: Random Forest, Logistic Regression, and XGBoost. All three models showed perfect or near-perfect accuracy again, with 100% classification rates, which is a strong indicator of data leakage. This happened because the yrbuilt column was still included as a feature after joining, which lets the models essentially memorize the answer.\n\nThe feature importances and coefficients confirmed this: yrbuilt was still the dominant driver in every model, overwhelming the effect of the new neighborhood variables. The added neighborhood features (like the nbhd_ variables) contributed almost nothing, showing extremely low or even zero importance scores.\n\nBecause of this, the results with the merged dataset do not actually change the recommended model. If we remove yrbuilt from the features, Random Forest would still likely perform best, similar to its ~80% accuracy from earlier runs. The neighborhood features might provide a small boost if the model is properly cleaned of leakage, but on their own they did not shift the model’s decision boundaries in a meaningful way.\n\nIn short, joining the neighborhood data did not meaningfully improve the models when yrbuilt was present, but could be helpful in the future if handled carefully and after removing data leakage.\n\n::: {#00ddf946 .cell execution_count=14}\n``` {.python .cell-code}\n# Stretch Task 2 (corrected - no leakage)\n\n# get neighborhood data from URL:\nneigh_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\ndwell_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\nneigh = pd.read_csv(neigh_url)\ndwell = pd.read_csv(dwell_url)\n\n# merge\njoined2 = dwell.merge(neigh, on=\"parcel\", how=\"left\")\n\n# target\ny2 = joined2[\"before1980\"]\n\n# drop leakage columns\nX2 = joined2.drop(joined2.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\nX2 = X2.fillna(0)\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(\n    X2, y2, test_size=0.2, stratify=y2, random_state=42\n)\n\n# 1. Random Forest\nrf2 = RandomForestClassifier(random_state=42)\nrf2.fit(X2_train, y2_train)\nprint_model_results(rf2, X2_test, y2_test, X2.columns)\n\n# 2. Logistic Regression\nlr2 = LogisticRegression(max_iter=1000)\nlr2.fit(X2_train, y2_train)\nprint_model_results(lr2, X2_test, y2_test, X2.columns)\n\n# 3. XGBoost\nxgb2 = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nxgb2.fit(X2_train, y2_train)\nprint_model_results(xgb2, X2_test, y2_test, X2.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRandomForestClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    2038      93\nActual 1      85    3377\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      2131\n           1       0.97      0.98      0.97      3462\n\n    accuracy                           0.97      5593\n   macro avg       0.97      0.97      0.97      5593\nweighted avg       0.97      0.97      0.97      5593\n\nFeature Importances:\nstories               6.727546e-02\narcstyle_ONE-STORY    6.392684e-02\nlivearea              6.347166e-02\nnumbaths              4.726956e-02\ngartype_Att           4.409021e-02\n                          ...     \nnbhd_214              3.969820e-08\nnbhd_659              3.945910e-08\nnbhd_653              1.889511e-08\nnbhd_648              1.610502e-08\ncondition_Fair        0.000000e+00\nLength: 321, dtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLogisticRegression Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1232     899\nActual 1     503    2959\n              precision    recall  f1-score   support\n\n           0       0.71      0.58      0.64      2131\n           1       0.77      0.85      0.81      3462\n\n    accuracy                           0.75      5593\n   macro avg       0.74      0.72      0.72      5593\nweighted avg       0.75      0.75      0.74      5593\n\nCoefficients:\narcstyle_ONE-STORY    0.079811\nnumbdrm               0.070719\ngartype_Det           0.068020\ncondition_Good        0.063046\nquality_C             0.048693\n                        ...   \nquality_B            -0.045697\ncondition_AVG        -0.068657\ngartype_Att          -0.070843\nstories              -0.074814\nnumbaths             -0.108668\nLength: 321, dtype: float64\n\nXGBClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    2018     113\nActual 1      77    3385\n              precision    recall  f1-score   support\n\n           0       0.96      0.95      0.96      2131\n           1       0.97      0.98      0.97      3462\n\n    accuracy                           0.97      5593\n   macro avg       0.97      0.96      0.96      5593\nweighted avg       0.97      0.97      0.97      5593\n\nFeature Importances:\narcstyle_ONE-STORY    0.307648\ngartype_Att           0.083395\nquality_C             0.069829\nnbhd_101              0.027231\nstories               0.025006\n                        ...   \nnbhd_531              0.000000\nnbhd_532              0.000000\nnbhd_533              0.000000\nnbhd_535              0.000000\nnbhd_523              0.000000\nLength: 321, dtype: float32\n```\n:::\n:::\n\n\n## STRETCH QUESTION|TASK 3\n\nFor this stretch question, I built a regression model to predict the year a house was built using a Random Forest Regressor. The model achieved a root mean squared error (RMSE) of about 11.6 years, meaning on average predictions were within roughly 12 years of the true build year. The median absolute error was lower, at around 3.9 years, which shows that half of the predictions were off by less than four years — a good sign that the model handles most houses reasonably well, with a few larger outliers.\n\nThe R² value was approximately 0.901, which means the model explained about 90% of the variance in the year built. Overall, this is a strong score for a regression problem with a complex target like construction year.\n\nWhile the random forest did a good job predicting year built, there is room to improve by adding more external features, such as neighborhood development data or historical zoning codes, to tighten the RMSE even further. Still, this model would provide a solid starting point for helping the client estimate missing build years when needed.\n\n::: {#d937234c .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Load your confirmed data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n\n# set target\ny_reg = df[\"yrbuilt\"]\n\n# drop target and identifier\nX_reg = df.drop(columns=[\"yrbuilt\", \"parcel\"])\nX_reg = X_reg.fillna(0)\n\n# split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\n# Random Forest Regressor\nrf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_reg.fit(X_train, y_train)\n\n# predict\ny_pred = rf_reg.predict(X_test)\n\n# evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))   # fixes the error\nr2 = r2_score(y_test, y_pred)\nmedae = median_absolute_error(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R²: {r2:.3f}\")\nprint(f\"Median Absolute Error: {medae:.2f}\")\n\n# residual plot\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color=\"red\")\nplt.xlabel(\"Predicted Year Built\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot\")\nplt.show()\n\n# feature importances\npd.Series(rf_reg.feature_importances_, index=X_reg.columns).sort_values().plot(\n    kind=\"barh\", figsize=(10,12)\n)\nplt.title(\"Feature Importances for Year Built Regression\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 11.60\nR²: 0.901\nMedian Absolute Error: 3.88\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-16-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](P4_template_files/figure-html/cell-16-output-3.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "P4_template_files"
    ],
    "filters": [],
    "includes": {}
  }
}