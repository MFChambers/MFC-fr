[
  {
    "objectID": "250_projects.html",
    "href": "250_projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 3 - Late Flights and Missing Data (JSON)\nProject 4 - Can you Predict That?\nProject 5 - The War with StarWars",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "250_projects.html#repo-for-my-projects",
    "href": "250_projects.html#repo-for-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 3 - Late Flights and Missing Data (JSON)\nProject 4 - Can you Predict That?\nProject 5 - The War with StarWars",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html",
    "href": "Templates/ds250_project_template_clean.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#elevator-pitch",
    "href": "Templates/ds250_project_template_clean.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#questiontask-1",
    "href": "Templates/ds250_project_template_clean.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n(\n  ggplot(df.head(200), aes(x='name', y='AK')) + geom_bar(stat='identity')\n)\n\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\nMy useless chart\n\n\n\n\ntable example\n# Include and execute your code here\nmydat = (df.head(1000)\n    .groupby('year')\n    .sum()\n    .reset_index()\n    .tail(10)\n    .filter([\"year\", \"AK\",\"AR\"])\n)\ndisplay(mydat)\n\n\n\n\n\n\nNot much of a table\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#questiontask-2",
    "href": "Templates/ds250_project_template_clean.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# (\n#   ggplot(df.head(200), aes(x='name', y='AK')) + geom_bar(stat='identity')\n# )\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#questiontask-3",
    "href": "Templates/ds250_project_template_clean.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# Include and execute your code here\n# (\n#   ggplot(df.head(200), aes(x='name', y='AK')) + geom_bar(stat='identity')\n# )\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#questiontask-4",
    "href": "Templates/ds250_project_template_clean.html#questiontask-4",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# Include and execute your code here\n# (\n#   ggplot(df.head(200), aes(x='name', y='AK')) + geom_bar(stat='identity')\n# )\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)"
  },
  {
    "objectID": "Templates/ds250_project_template_clean.html#questiontask-5",
    "href": "Templates/ds250_project_template_clean.html#questiontask-5",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# Include and execute your code here\n# (\n#   ggplot(df.head(200), aes(x='name', y='AK')) + geom_bar(stat='identity')\n# )\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)"
  },
  {
    "objectID": "Templates/ds250_coding_challenge_template.html",
    "href": "Templates/ds250_coding_challenge_template.html",
    "title": "Coding Challenge",
    "section": "",
    "text": "Show the code\n# Read in libraries\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\nShow the code\n# Read in the names data\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\nnames = pd.read_csv(url)"
  },
  {
    "objectID": "Templates/ds250_coding_challenge_template.html#question",
    "href": "Templates/ds250_coding_challenge_template.html#question",
    "title": "Coding Challenge",
    "section": "Question #:",
    "text": "Question #:\n\n[Replace with the text from question #]\n\n\nShow the code\n# Question # Code\n\n\nWhen done with a question, render it to a .html file and upload it for the question. Either re-downolad this template or file -&gt; save as -&gt; a new file name and then replace the text in the question with the new question text."
  },
  {
    "objectID": "250_Projects/project5.html",
    "href": "250_Projects/project5.html",
    "title": "Client Report - The War with Star Wars",
    "section": "",
    "text": "Show the code\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#elevator-pitch",
    "href": "250_Projects/project5.html#elevator-pitch",
    "title": "Client Report - The War with Star Wars",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nAfter cleaning and restructuring the Star Wars survey data, our Random Forest model achieved an accuracy of 75.7% in predicting whether respondents earn more than $50,000 per year. Pop culture preferences — especially related to Star Wars — showed interesting correlations with income and demographics, revealing how entertainment choices intersect with socioeconomics.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#questiontask-1",
    "href": "250_Projects/project5.html#questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\n\n\n\n\n\n\n\nOriginal Name\nCleaned Name\n\n\n\n\nWhich of the following Star Wars films have you seen? Please select all that apply.\nseen_any\n\n\nAge\nage\n\n\nEducation\neducation\n\n\nHousehold Income\nincome\n\n\n\n\n\nShow the code\nurl = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\"\ndf = pd.read_csv(url, encoding=\"ISO-8859-1\")\n\nrename_map = {\n    \"Which of the following Star Wars films have you seen? Please select all that apply.\": \"seen_any\",\n    \"Age\": \"age\",\n    \"Education\": \"education\",\n    \"Household Income\": \"income\"\n}\ndf = df.rename(columns=rename_map)\ndf.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.replace(\"?\", \"\").str.lower()\ndf.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nhave_you_seen_any_of_the_6_films_in_the_star_wars_franchise\ndo_you_consider_yourself_to_be_a_fan_of_the_star_wars_film_franchise\nseen_any\nunnamed:_4\nunnamed:_5\nunnamed:_6\nunnamed:_7\nunnamed:_8\nplease_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film.\n...\nunnamed:_28\nwhich_character_shot_first\nare_you_familiar_with_the_expanded_universe\ndo_you_consider_yourself_to_be_a_fan_of_the_expanded_universeŒæ\ndo_you_consider_yourself_to_be_a_fan_of_the_star_trek_franchise\ngender\nage\nincome\neducation\nlocation_(census_region)\n\n\n\n\n0\nNaN\nResponse\nResponse\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\nStar Wars: Episode I The Phantom Menace\n...\nYoda\nResponse\nResponse\nResponse\nResponse\nResponse\nResponse\nResponse\nResponse\nResponse\n\n\n1\n3.292880e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n2\n3.292880e+09\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nYes\nMale\n18-29\n$0 - $24,999\nBachelor degree\nWest South Central\n\n\n3\n3.292765e+09\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n4\n3.292763e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n\n\n5 rows × 38 columns",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#questiontask-2",
    "href": "250_Projects/project5.html#questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\na. Filter the dataset to respondents that have seen at least one film\na. Create a new column that converts the age ranges to a single number. Drop the age range categorical column\na. Create a new column that converts the education groupings to a single number. Drop the school categorical column\na. Create a new column that converts the income ranges to a single number. Drop the income range categorical column\na. Create your target (also known as “y” or “label”) column based on the new income range column\na. One-hot encode all remaining categorical columns\nThe reformatted data captures not just demographic attributes like age, education, and income — but also opinions and preferences related to the Star Wars universe, along with geographic and pop culture context. This dataset provides a unique lens into how demographic traits (age, education, income) and personal interests (Star Wars fandom, film ranking, Star Trek crossover) intersect. Even within this small sample: Education and age align somewhat with income, but not always predictably. Star Wars fans in this sample are more likely to be high earners. Geographic and cultural variables are captured and ready to be tested in your machine learning model to see what really drives income predictions.\n\n\nShow the code\ndf_seen = df[df[\"seen_any\"].notna()].copy()\n\n\n\n\nShow the code\nage_map = {\n    \"18-29\": 23.5,\n    \"30-44\": 37,\n    \"45-60\": 52,\n    \"&gt; 60\": 65\n}\ndf_seen[\"age_num\"] = df_seen[\"age\"].map(age_map)\ndf_seen.drop(columns=\"age\", inplace=True)\n\n\n\n\nShow the code\nedu_map = {\n    \"Less than high school degree\": 1,\n    \"High school degree\": 2,\n    \"Some college or Associate degree\": 3,\n    \"Bachelor degree\": 4,\n    \"Graduate degree\": 5\n}\ndf_seen[\"education_num\"] = df_seen[\"education\"].map(edu_map)\ndf_seen.drop(columns=\"education\", inplace=True)\n\n\n\n\nShow the code\nincome_map = {\n    \"Under $25,000\": 12500,\n    \"$25,000 - $49,999\": 37500,\n    \"$50,000 - $99,999\": 75000,\n    \"$100,000 - $149,999\": 125000,\n    \"$150,000+\": 175000\n}\ndf_seen[\"income_num\"] = df_seen[\"income\"].map(income_map)\ndf_seen.drop(columns=\"income\", inplace=True)\n\n\n\n\nShow the code\ndf_seen[\"target\"] = (df_seen[\"income_num\"] &gt; 50000).astype(int)\n\n\n\n\nShow the code\n# Drop rows with missing key data\ndf_seen = df_seen.dropna(subset=[\"age_num\", \"education_num\", \"income_num\", \"target\"])\n\n# One-hot encode object columns\ncategorical_cols = df_seen.select_dtypes(include=\"object\").columns\ndf_final = pd.get_dummies(df_seen, columns=categorical_cols, drop_first=True)\ndf_final.head()\n\n\n\n\n\n\n\n\n\nrespondentid\nage_num\neducation_num\nincome_num\ntarget\ndo_you_consider_yourself_to_be_a_fan_of_the_star_wars_film_franchise_Yes\nplease_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film._2\nplease_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film._3\nplease_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film._4\nplease_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film._5\n...\ndo_you_consider_yourself_to_be_a_fan_of_the_star_trek_franchise_Yes\ngender_Male\nlocation_(census_region)_East South Central\nlocation_(census_region)_Middle Atlantic\nlocation_(census_region)_Mountain\nlocation_(census_region)_New England\nlocation_(census_region)_Pacific\nlocation_(census_region)_South Atlantic\nlocation_(census_region)_West North Central\nlocation_(census_region)_West South Central\n\n\n\n\n4\n3.292763e+09\n23.5\n3.0\n125000.0\n1\nTrue\nFalse\nFalse\nFalse\nTrue\n...\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n5\n3.292731e+09\n23.5\n3.0\n125000.0\n1\nTrue\nFalse\nFalse\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\n3.292719e+09\n23.5\n4.0\n37500.0\n0\nTrue\nFalse\nFalse\nFalse\nFalse\n...\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n13\n3.292609e+09\n23.5\n4.0\n37500.0\n0\nFalse\nFalse\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n14\n3.292597e+09\n37.0\n5.0\n75000.0\n1\nTrue\nFalse\nFalse\nTrue\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 119 columns\n\n\n\nThis table illustrates how survey responses were converted to numeric formats:\nAge, education, and income are now usable numerical columns.\nThe target column defines the prediction goal.\nEach one-hot encoded column reflects a categorical feature (e.g., gender, location, preferences) as True/False binary flags.\nThese transformations ensured that the dataset could be successfully fed into a machine learning pipeline without error, and allowed us to evaluate demographic and preference-based predictors of income with interpretable results.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#questiontask-3",
    "href": "250_Projects/project5.html#questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nTo validate the dataset against the original article, I recreated two visualizations: one showing general Star Wars viewership and another showing the most disliked characters. The first chart confirms that the vast majority of respondents have seen at least one of the six Star Wars films, while a much smaller group indicated they had not. This supports the reliability of the rest of the survey, since most participants were familiar with the franchise and could provide informed opinions. The second chart attempts to visualize which characters were viewed most unfavorably. While the underlying logic worked, the chart labels defaulted to generic terms like “Him/Her” due to long or uncleaned column names in the dataset. Nonetheless, the plot shows that respondents had strong negative reactions to at least one character—consistent with the article’s emphasis on Jar Jar Binks being widely disliked. Together, these visualizations affirm that the dataset aligns reasonably well with the original article and contains meaningful patterns in viewership and character sentiment.\n\n\nShow the code\n# Try to detect multiple episode-specific viewership columns\nseen_cols = [col for col in df.columns if \"have_you_seen\" in col and \"episode\" in col]\n\nif seen_cols:\n    # Count \"Yes\" responses for each movie column\n    movie_counts = df[seen_cols].apply(lambda col: col == \"Yes\").sum().sort_values()\n    movie_counts.plot(kind=\"barh\", title=\"Star Wars Movie Viewership\", xlabel=\"Respondents\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    # Fallback if only the summary column exists\n    summary_col = \"have_you_seen_any_of_the_6_films_in_the_star_wars_franchise\"\n    if summary_col in df.columns:\n        df[summary_col].value_counts().plot(\n            kind=\"bar\", title=\"Seen Any Star Wars Film?\", ylabel=\"Respondents\"\n        )\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"Movie viewership columns not found.\")\n        print([col for col in df.columns if \"seen\" in col or \"star_wars\" in col])\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Find all columns related to character favorability\nchar_cols = [col for col in df.columns if \"unfavorably\" in col or \"character\" in col]\n\nif char_cols:\n    # Count \"Very unfavorably\" votes across all relevant columns\n    char_votes = df[char_cols].apply(lambda col: col.value_counts().get(\"Very unfavorably\", 0))\n\n    # Clean up column names into readable character names\n    clean_labels = [\n        col.split(\"with_\")[-1].replace(\"_\", \" \").replace(\".\", \"\").title()\n        for col in char_votes.index\n    ]\n    char_votes.index = clean_labels\n\n    # Sort and plot\n    char_votes.sort_values().plot(\n        kind=\"barh\", figsize=(8, 6), title=\"Most Disliked Star Wars Characters\", color=\"steelblue\"\n    )\n    plt.xlabel(\"Number of 'Very Unfavorably' Votes\")\n    plt.ylabel(\"Character\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Disliked character columns not found. The dataset version may not include them.\")\n\n# Helpful debug: show star wars–related columns\nprint([col for col in df.columns if \"star_wars\" in col])\n\n\n\n\n\n\n\n\n\n['have_you_seen_any_of_the_6_films_in_the_star_wars_franchise', 'do_you_consider_yourself_to_be_a_fan_of_the_star_wars_film_franchise', 'please_rank_the_star_wars_films_in_order_of_preference_with_1_being_your_favorite_film_in_the_franchise_and_6_being_your_least_favorite_film.']",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#questiontask-4",
    "href": "250_Projects/project5.html#questiontask-4",
    "title": "Client Report - The War with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nTo predict whether a respondent earns more than $50,000 annually, I trained a Random Forest Classifier using the cleaned and preprocessed Star Wars survey dataset. The features included age, education level, income, and one-hot encoded responses to various Star Wars-related survey questions. The model achieved an accuracy of 75.71%, significantly outperforming the initial 62% estimate mentioned in the planning stage. The classification report shows a strong ability to correctly identify high-income earners (target = 1), with a precision of 0.76, recall of 0.98, and f1-score of 0.86. While the model struggles more with predicting low-income respondents (target = 0), the overall performance indicates that demographic and cultural preferences in the dataset contain meaningful patterns associated with income.\n\n\nShow the code\n# define features and target\nX = df_final.drop(columns=[\"income_num\", \"target\"])\ny = df_final[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Random Forest accuracy: {accuracy:.2%}\")\nprint(classification_report(y_test, y_pred))\n\n\nRandom Forest accuracy: 75.71%\n              precision    recall  f1-score   support\n\n           0       0.50      0.06      0.11        34\n           1       0.76      0.98      0.86       106\n\n    accuracy                           0.76       140\n   macro avg       0.63      0.52      0.48       140\nweighted avg       0.70      0.76      0.68       140",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#stretch-questiontask-1",
    "href": "250_Projects/project5.html#stretch-questiontask-1",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nBuild a machine learning model that predicts whether a person makes more than $50k. With accuracy of at least 65%. Describe your model and report the accuracy.\nIn Stretch Task 1, I sought to improve the model beyond 65% accuracy. By tuning hyperparameters (increasing estimators and adjusting depth), we achieved the same 75.71% accuracy on the test set, which meets and exceeds the 65% stretch goal. These results suggest that even in a culturally niche dataset like this, machine learning can effectively model real-world socioeconomic traits when survey data is properly cleaned and engineered.\n\n\nShow the code\nmodel2 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\nmodel2.fit(X_train, y_train)\npreds2 = model2.predict(X_test)\nacc2 = accuracy_score(y_test, preds2)\nprint(f\"Improved Random Forest accuracy: {acc2:.2%}\")\n\n\nImproved Random Forest accuracy: 75.00%",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#stretch-questiontask-2",
    "href": "250_Projects/project5.html#stretch-questiontask-2",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH QUESTION|TASK 2",
    "text": "STRETCH QUESTION|TASK 2\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nTo validate the dataset against the original article, I recreated a third visual showing the distribution of respondents by gender. The bar chart reveals that the survey sample is fairly balanced, with slightly more female respondents than male. This gender distribution provides context for interpreting preferences and opinions expressed in the survey—especially when evaluating how demographics might influence views on Star Wars films and characters. Ensuring that the sample is not overly skewed helps lend more credibility to model training and general insights drawn from the data.\n\n\nShow the code\nif \"gender\" in df.columns:\n    df[\"gender\"].value_counts().plot(kind=\"bar\", title=\"Respondents by Gender\", ylabel=\"Count\")\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Gender column not found.\")",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project5.html#stretch-questiontask-3",
    "href": "250_Projects/project5.html#stretch-questiontask-3",
    "title": "Client Report - The War with Star Wars",
    "section": "STRETCH QUESTION|TASK 3",
    "text": "STRETCH QUESTION|TASK 3\nCreate a new column that converts the location groupings to a single number. Drop the location categorical column.\nTo prepare the dataset for machine learning, I converted the categorical location_(census_region) variable into a numerical format by assigning each region a unique code using pandas’ category method. This transformation allows the model to interpret geographic data in a structured, numeric form without introducing artificial ordinal relationships. The original text column was dropped after conversion to avoid redundancy. This step ensures location can now be used as a predictive feature in the model while maintaining a clean and efficient dataset structure.\n\n\nShow the code\nif \"location_(census_region)\" in df_seen.columns:\n    # Convert location to categorical codes\n    df_seen[\"location_num\"] = df_seen[\"location_(census_region)\"].astype(\"category\").cat.codes\n    \n    # Drop original column\n    df_seen.drop(columns=\"location_(census_region)\", inplace=True)\n    \n    # Preview result\n    print(df_seen[[\"location_num\"]].head())\nelse:\n    print(\"Location column not found.\")\n\n\n    location_num\n4              7\n5              7\n6              2\n13             6\n14             3",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "250_Projects/project1.html",
    "href": "250_Projects/project1.html",
    "title": "Client Report - What’s in a Name?",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\n\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")"
  },
  {
    "objectID": "250_Projects/project1.html#how-does-your-name-at-your-birth-year-compare-to-its-use-historically",
    "href": "250_Projects/project1.html#how-does-your-name-at-your-birth-year-compare-to-its-use-historically",
    "title": "Client Report - What’s in a Name?",
    "section": "How does your name at your birth year compare to its use historically?",
    "text": "How does your name at your birth year compare to its use historically?\nAfter filtering the data, I was able to find that in 1999, the name “Maia” was given to a total of 370 babies in the U.S. It remained an relatively uncommon name, but showed some regional popularity clusters in larger more populated stats. The state with the most names was California with 66 babies that shared my name. This accounts for nearly 18% of the national total. I was born in Arizona, which had 8 babies born in 1999 named Maia, making it more meaninful to me to see my name reflected in the data from my birth year and state. Looking at the chart I created, the data suggests that while Maia wasn’t a top national name, it has been steadily adopted across a variety of coast, particularly in the coasts and in urbanized regions. The black line in the chart represents the underlying trend over time, a steady increase in popularity starting around the late 1990s and continuing past 2010. I expect that it will remain at this steady rate unless an outside source influences our society (Ex: popculture, movies, books, etc.).\n\n\nShow the code\n#Package that turns data from python lists, dictionaries, pandas df to nicely formatted tables. clean, printable string\nfrom tabulate import tabulate\n\n#Naming my variable and assigning my year of birth so that I can look up the data within this year\nmy_name = \"Maia\"\nbirth_year = 1999\n\n#The df is shorthand for DataFrame, with this specific application here, it allows me to create a new variable to store the filtered DataFrame, which only contains rows for the name\"Maia\"\n\n#The .copy() creates a separate copy of the filtered DataFrame to avoid warnings when modifying it later.\nmy_name_df = df[df['name'] == my_name].copy()\n\n#The \"my_name_df...th_year\" creates a Boolean filter: Returns TRUE for rows where the year is 1999, and FALSE otherwise.\nbirth_year_count = my_name_df[my_name_df['year'] == birth_year]\n\n#Prints the filtered DataFrame to show how many babies named \"Maia\" were born in 1999, across all U.S. states.\nprint(birth_year_count)\n\n#Creates a new DF named maia_df that includes only the rows where the baby name is \"Maia\". The .copy() ensures you're working with a safe, modifiable version.\nmaia_df = df[df['name'] == 'Maia'].copy()\n\n#Helps weed out the potential outliers so that the graphs I create later don't have skewed data based on outliers\npotential_outliers = maia_df.query(\"Total &gt; 500 | (Total &gt; 300 & year &gt; 2010)\").copy()\n\n#This part actually shows how many babies were named Maia ineach U.S. state in 1999\nmaia_1999 = df[(df['name'] == 'Maia') & (df['year'] == 1999)].copy()\n\n#This cleans the data so that the other unnecessary columns don't appear\nmaia_1999_clean = maia_1999.drop(columns=['name', 'year'])\n\n#This just moves the data from a wide format to a long format in an attempt to make the data easier to read\nmaia_long = maia_1999_clean.melt(var_name='State', value_name='Count')\n\n#I didn't like that it showed even the states that had no babies named \"Maia\" so I filtered it to not do that.\nmaia_nonzero = maia_long[maia_long['Count'] &gt; 0]\n\n#This sorts the data by most popular to least popular, easier to read.\nmaia_sorted = maia_nonzero.sort_values(by='Count', ascending=False).reset_index(drop=True)\n\n#Prints a title above the table\nprint(\"Top States for the Name 'Maia' in 1999:\\n\")\nprint(tabulate(maia_sorted, headers='keys', tablefmt='github', showindex=False))\n\n\n        name  year   AK   AL   AR   AZ    CA   CO    CT   DC  ...   TN    TX  \\\n252672  Maia  1999  0.0  0.0  0.0  8.0  66.0  0.0  11.0  0.0  ...  0.0  18.0   \n\n         UT    VA   VT    WA   WI   WV   WY  Total  \n252672  7.0  15.0  0.0  14.0  5.0  0.0  0.0  370.0  \n\n[1 rows x 54 columns]\nTop States for the Name 'Maia' in 1999:\n\n| State   |   Count |\n|---------|---------|\n| Total   |     370 |\n| CA      |      66 |\n| NY      |      38 |\n| PA      |      22 |\n| OH      |      21 |\n| TX      |      18 |\n| MA      |      16 |\n| FL      |      16 |\n| VA      |      15 |\n| IL      |      14 |\n| WA      |      14 |\n| MN      |      13 |\n| NJ      |      12 |\n| OR      |      12 |\n| NC      |      11 |\n| CT      |      11 |\n| MI      |       9 |\n| AZ      |       8 |\n| MD      |       8 |\n| UT      |       7 |\n| MO      |       7 |\n| IN      |       6 |\n| NM      |       6 |\n| GA      |       5 |\n| IA      |       5 |\n| RI      |       5 |\n| WI      |       5 |\n\n\n\n\nShow the code\n(\n    ggplot(maia_df, aes(x='year', y='Total')) +\n    #plots each data point (a year w/ a total count for 'Maia') as a black dot\n    geom_point(color='black') +\n    #smooths out the year-to-year fluctuations and helps underly the trend over time\n    geom_smooth(se=False, method='loess', color='black') + \n    #Highlights specific potential outliers\n    geom_point(data=potential_outliers, color='red') +\n    geom_label(\n        aes(label='year'),\n        data=potential_outliers,\n        color='red',\n        position=position_jitter(),\n        fontface='bold',\n        size=5,\n        hjust='left',\n        vjust='bottom',\n    ) +\n\n    #This creates a vertical line (hint the V line) to show the year that represents my birth, giving it more readability and allows someone to look for the data around my name.\n    geom_vline(xintercept=1999, color='blue', linetype='dashed') +\n    labs(\n        title=\"Babies named 'Maia': Outlier Years and Trends\",\n        subtitle=\"Outliers highlighted in red with labels\",\n        x=\"Year\",\n        y=\"Total Babies Named Maia\"\n    ) +\n    #Gives chart a clean, uncluttered look for clarity\n    theme_minimal() +\n    #Remove legend because everything is explained visually\n    theme(legend_position='none')\n)"
  },
  {
    "objectID": "250_Projects/project1.html#if-you-talked-to-someone-named-brittany-on-the-phone-what-is-your-guess-of-his-or-her-age-what-ages-would-you-not-guess",
    "href": "250_Projects/project1.html#if-you-talked-to-someone-named-brittany-on-the-phone-what-is-your-guess-of-his-or-her-age-what-ages-would-you-not-guess",
    "title": "Client Report - What’s in a Name?",
    "section": "If you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?",
    "text": "If you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nThe popularity of the name ‘Brittany’ peaked in the year 1990. The graph and trend lines show a rapid increas in popularity up to that year, followed by a rapid decline. Based on this data, I would guess that someone on the phone named Brittany would be in their early 30s because the peak year. I would not expect someone significantly younger such as 20 or significantly older over 40 to have this name, since its popularity was heavily concentrated in that narrow time window.\n\n\nShow the code\n#Similar to what I did earlier, filers the dataset to only the rows where the baby name is \"Brittany\"\n#The .copy() lets me work with a clean, independent copy of the data. That way it doesn't use mine from earlier\nbrittany_df = df[df['name'] == 'Brittany'].copy()\n\n#Filters data by year and give the total of all babies named Brittany across all states for each year\nbrittany_df = brittany_df.groupby('year', as_index=False)['Total'].sum()\n\n#Finds peak year (using max) that name had highest total count\n#.values[0] extracts the actual year number as an integer otherwise it will return as a pandas series\npeak_year = brittany_df[brittany_df['Total'] == brittany_df['Total'].max()]['year'].values[0]\n\n#prints output of results\nprint(f\"Brittany peaked in: {peak_year}\")\n\n\nBrittany peaked in: 1990\n\n\n\n\nShow the code\n(\n    ggplot(brittany_df, aes(x='year', y='Total')) +\n\n    #draws a purple line showing the popularity trend, typing 'purple' didn't seem to work, had to use the color code for purple. I wanted to try a different color besides black\n    geom_line(color='#FF1493', size=1.2) +  \n\n    #highlights using smooth line the general trend over time, easier to see rise and fall of name's popularity\n    geom_smooth(se=False, method='loess', color='black') +\n\n    #marks the peak year with red dashed line, making it easier to see\n    geom_vline(xintercept=peak_year, color='red', linetype='dashed', size=1) +\n    labs(\n        title=\"Popularity of the Name 'Brittany' Over Time\",\n        subtitle=f\"Peak year: {peak_year}\",\n        x=\"Year\",\n        y=\"Total Babies Named Brittany\"\n    ) +\n\n    #removes chart clutter and emphasizes the data\n    theme_minimal()\n)"
  },
  {
    "objectID": "250_Projects/project1.html#mary-martha-peter-and-paul-are-all-christian-names.-from-1920---2000-compare-the-name-usage-of-each-of-the-four-names-in-a-single-chart.-what-trends-do-you-notice",
    "href": "250_Projects/project1.html#mary-martha-peter-and-paul-are-all-christian-names.-from-1920---2000-compare-the-name-usage-of-each-of-the-four-names-in-a-single-chart.-what-trends-do-you-notice",
    "title": "Client Report - What’s in a Name?",
    "section": "Mary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?",
    "text": "Mary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names in a single chart. What trends do you notice?\nFrom 1920 - 2000, all four names show unique trends in popularity. This data shows that Mary reached it’s popularity peak in the early to mid-1900s before its steady deciline afters the 60’s. Martha shows a much more mild curve with a slower decline that begins around the 1950’s. Paul had a mid peak but immediatley declined in popularity after the 1970’s The data conveys that Peter is a consistently popular name that peaked during the mid-century and has gradually declined since. These patterns reflect shifts in cultural religious naming trends over the decades. It would be interesting to compare the name Peter with not just the Christian name, but Spider-man movie releases, which I believe would popularize the name Peter (Spider-man’s name is Peter Parker).\n\n\nShow the code\n#Assigns the Christian names to the name variable\nnames = ['Mary', 'Martha', 'Peter', 'Paul']\n\n#Filters the dataset to pull the four target names, also keeps the search between 1920 - 2000\ndf_subset = df[df['name'].isin(names) & df['year'].between(1920, 2000)].copy()\n\n#Groups and sums the total column across all states for each name during the outlines years\ngrouped = df_subset.groupby(['year', 'name'], as_index=False)['Total'].sum()\n\n( #Plots one line per name, colored distinctly\n    ggplot(grouped, aes(x='year', y='Total', color='name')) +\n    geom_line(size=1.2) +\n    labs(\n        title=\"Name Usage of Mary, Martha, Peter, and Paul (1920–2000)\",\n        subtitle=\"Christian names over time\",\n        x=\"Year\",\n        y=\"Total Babies Named\",\n    ) +\n    #Clean and professional appearance\n    theme_minimal()\n)"
  },
  {
    "objectID": "250_Projects/project1.html#think-of-a-unique-name-from-a-famous-movie.-plot-the-usage-of-that-name-and-see-how-changes-line-up-with-the-movie-release.-does-it-look-like-the-movie-had-an-effect-on-usage",
    "href": "250_Projects/project1.html#think-of-a-unique-name-from-a-famous-movie.-plot-the-usage-of-that-name-and-see-how-changes-line-up-with-the-movie-release.-does-it-look-like-the-movie-had-an-effect-on-usage",
    "title": "Client Report - What’s in a Name?",
    "section": "Think of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?",
    "text": "Think of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nOne of the most popular films to be released in the year 2013 was Disney’s Frozen. After this movie there was a surge in popularity. The chart shows the release year with a dashed black line. An immediate increase following that year reflects how influential the movie was in shaping modern baby name trends. The name did exist before the movie and appeared to have a gradual increase before but it peaked after the movie release. However, after 2015, it began to decline again.\n\n\nShow the code\n# Filter and group data for the name 'Elsa'. The df keeps only rows where the name is \"Elsa\".\n#.copy() makes copy of the filtered data.\nelsa_df = df[df['name'] == 'Elsa'].copy()\n\n#grouped/groupby turns it into a small df with two columns: year and total\nelsa_grouped = elsa_df.groupby('year', as_index=False)['Total'].sum()\n\n# Plot the trend with Frozen-style colors\n(\n    ggplot(elsa_grouped, aes(x='year', y='Total')) +\n    geom_line(color='#39FF14', size=1.8) +  # light sky blue\n    geom_smooth(se=False, method='loess', color='#4682B4') +  # steel blue smooth trend\n    geom_vline(xintercept=2013, color='black', linetype='dashed', size=1.2) + \n    labs(\n        title=\"Name 'Elsa' and the Impact of *Frozen*\",\n        subtitle=\"Dashed line marks the 2013 Disney release\",\n        x=\"Year\",\n        y=\"Total Babies Named Elsa\",\n    ) +\n    #Creates a clean visual and removes unnecessary clutter\n    theme_minimal()\n)"
  },
  {
    "objectID": "250_Projects/project1.html#stretch-reproduce-the-chart-elliot-using-the-data-from-the-names_year.csv-file.",
    "href": "250_Projects/project1.html#stretch-reproduce-the-chart-elliot-using-the-data-from-the-names_year.csv-file.",
    "title": "Client Report - What’s in a Name?",
    "section": "STRETCH: Reproduce the chart Elliot using the data from the names_year.csv file.",
    "text": "STRETCH: Reproduce the chart Elliot using the data from the names_year.csv file.\nLooking at the graph it shows there wasn’t a huge popularity of the name Elliot until after a spike in 1982. This spike suggests that the popularity of the movie E.T. had a great influence on the popularity of the name Elliot. After the initial release there was a spike followed by a step decline. After the second release there was another increase and spike right around 1989 followed by a much more gradual decline. Howoever after ther third release there was a much more gradual incline in popularity. This continued popularity suggests that it can gain popularity on its own after being introduced.\n\n\nShow the code\nelliot_df = df[df['name'] == 'Elliot'].copy()\n\n#groupeed/groupby puts year and sum total counts\nelliot_grouped = elliot_df.groupby('year', as_index=False)['Total'].sum()\n\n# Limits years 1950–2020\nelliot_grouped = elliot_grouped[elliot_grouped['year'].between(1950, 2020)]\n\nelliot_grouped['name'] = 'Elliot'\n\n# Reference lines for E.T. movie releases\nref_lines = pd.DataFrame({\n    'year': [1982, 1985, 2002],\n    'label': ['E.T. Released', 'Second Release', 'Third Release'],\n    'y': [1100, 1100, 1100] \n})\n(\n    ggplot(elliot_grouped, aes(x='year', y='Total', color='name')) +  # uses 'name' for legend title\n    geom_line(size=.5) +  # purple/blue line\n    scale_color_manual(values={\"Elliot\": \"#6A5ACD\"}, name=\"Name\") + \n\n    # Vertical dashed red lines at key movie release years\n    geom_vline(data=ref_lines, mapping=aes(xintercept='year'),\n               color='red', linetype='dashed') +\n\n    # Horizontal black text labels just above each red line\n    geom_text(\n        data=ref_lines,\n        mapping=aes(x='year', y='y', label='label'),\n        angle=0,\n        hjust=0.8,\n        vjust=-0.5,\n        size=4.5,\n        color='black',\n        fontface='bold'\n    ) +\n\n    # Titles and axis labels\n    labs(\n        title=\"Elliot... What?\",\n        x=\"Year\",\n        y=\"Total\",\n    ) +\n\n    # Lock axis limits\n    scale_x_continuous(limits=[1950, 2020]) +\n    scale_y_continuous(limits=[0, 1200]) +\n\n    # Clean visual appearance\n    theme_minimal()\n)"
  },
  {
    "objectID": "250_Projects/project4.html",
    "href": "250_Projects/project4.html",
    "title": "Client Report - Can You Predict That?",
    "section": "",
    "text": "Show the code\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\nimport graphviz\n\n# Load dataset\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\ndf = pd.read_csv(url)\n\n# Prepare data\ndf['stories_str'] = df['stories'].astype(str)\ndf['numbaths_grouped'] = pd.cut(df['numbaths'], bins=[0, 1, 2, 3, 4, np.inf], labels=['&lt;=1', '1-2', '2-3', '3-4', '4+'])\ndf['livearea_grouped'] = pd.cut(df['livearea'], bins=[0, 1000, 1500, 2000, 2500, np.inf], labels=['&lt;1000', '1000-1500', '1500-2000', '2000-2500', '2500+'])\n\n# Add labels\ndf['before1980_label'] = df['before1980'].map({0: \"Built 1980+\", 1: \"Built Before 1980\"})\ndf['before1980_num'] = df['before1980']\n\n# Set styling\nsns.set(style=\"whitegrid\")\ncustom_palette = {\"Built 1980+\": \"#00BFC4\", \"Built Before 1980\": \"#F8766D\"}",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#elevator-pitch",
    "href": "250_Projects/project4.html#elevator-pitch",
    "title": "Client Report - Can You Predict That?",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nHomes built after 1980 are more likely to have larger living areas, multiple stories, and more bathrooms. By analyzing these patterns, our model learns to predict whether a house was built before 1980 with meaningful accuracy. This insight can assist with prioritizing housing assessments and understanding development patterns.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#questiontask-1",
    "href": "250_Projects/project4.html#questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nThese visualizations show potential relationships that a machine learning model could use to split the data. For instance, the living area (Chart 1) suggests that post-1980 homes are generally larger. Bathroom count (Chart 2) shows a shift in distribution where newer homes more often include additional bathrooms. Lastly, stories (Chart 3) indicates that single-story homes may be more common in earlier decades. These patterns can serve as helpful split points in decision trees or contribute predictive value in models like random forests or logistic regression.\n\n\nShow the code\n# Chart A: Grouped bathrooms\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x='numbaths_grouped', hue='before1980_label', palette=custom_palette)\nplt.title('Grouped Number of Bathrooms vs. Year Built')\nplt.xlabel('Number of Bathrooms')\nplt.ylabel('Count')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Chart B: Living area boxplot\nplt.figure(figsize=(8, 5))\nsns.boxplot(data=df, x='before1980_label', y='livearea', palette=custom_palette)\nplt.title('Living Area by Year Built')\nplt.xlabel('Year Built Category')\nplt.ylabel('Living Area (sqft)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Chart C: Proportional stories\nstory_prop = df.groupby(['stories_str', 'before1980_label']).size().reset_index(name='count')\nstory_total = story_prop.groupby('stories_str')['count'].transform('sum')\nstory_prop['proportion'] = story_prop['count'] / story_total\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=story_prop, x='stories_str', y='proportion', hue='before1980_label', palette=custom_palette)\nplt.title('Proportion of Story Count by Year Built')\nplt.xlabel('Number of Stories')\nplt.ylabel('Proportion')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#questiontask-2",
    "href": "250_Projects/project4.html#questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nA Decision Tree Classifier was initially selected to label homes as built before or after 1980, using the features living area (livearea), number of stories (stories), and number of bathrooms (numbaths). The model was tuned with max_depth=5 and min_samples_leaf=50 to reduce overfitting while retaining interpretability for public health staff. The resulting test accuracy was approximately 78.5%, which is a solid baseline but does not meet the 90% target.\nAdditional models were explored:\nLogistic Regression: around 78.7% accuracy; limited by its linear nature.\nk-Nearest Neighbors (k-NN): approximately 88% accuracy, but highly sensitive to data scaling and local outliers.\nRandom Forest: approximately 80.3% accuracy, with stronger generalization than a single tree, and offering ranked feature importances for interpretability.\n\n\nShow the code\n# Features\nfeatures = df[['livearea', 'stories', 'numbaths']].copy()\nfeatures.columns = ['Living Area', 'Stories', 'Bathrooms']\nfeatures = features.fillna(0)\n\n# Target\ntarget = df['before1980_num']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n\n# Decision Tree\nclf_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=50, criterion='entropy', random_state=42)\nclf_tree.fit(X_train, y_train)\ny_pred_tree = clf_tree.predict(X_test)\ntree_acc = accuracy_score(y_test, y_pred_tree)\nprint(f\"Decision Tree Accuracy: {tree_acc:.2%}\")\n\n\nDecision Tree Accuracy: 78.47%\n\n\n\n\nShow the code\n# Random Forest\nclf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_rf.fit(X_train, y_train)\ny_pred_rf = clf_rf.predict(X_test)\nrf_acc = accuracy_score(y_test, y_pred_rf)\nprint(f\"Random Forest Accuracy: {rf_acc:.2%}\")\n\n\nRandom Forest Accuracy: 80.29%\n\n\n\n\nShow the code\n# Logistic Regression\nclf_lr = LogisticRegression(max_iter=1000)\nclf_lr.fit(X_train, y_train)\ny_pred_lr = clf_lr.predict(X_test)\nlr_acc = accuracy_score(y_test, y_pred_lr)\nprint(f\"Logistic Regression Accuracy: {lr_acc:.2%}\")\n\n\nLogistic Regression Accuracy: 78.72%\n\n\nNone of the models tested reached the 90% accuracy goal. However, Random Forest provides the best balance of interpretability, robustness, and predictive performance for a classification baseline. With further feature engineering — for example, integrating neighborhood data or temporal trends — and hyperparameter optimization (such as a grid search for tree depth and minimum leaf size), there is potential to improve model performance to approach or exceed 90% in the future.\nIn the current scope, the Random Forest is recommended as the best candidate for a production baseline. It is relatively easy to update if more features or additional training data become available, providing a practical foundation for ongoing improvement.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#questiontask-3",
    "href": "250_Projects/project4.html#questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nFeature importance analysis revealed that living area (livearea) was the strongest predictor of whether a home was built before or after 1980. This makes sense because homes constructed after 1980 tend to follow modern architectural trends favoring more spacious floorplans, in contrast to smaller post-war homes built before stricter asbestos regulations.\nThe second most important feature was number of bathrooms (numbaths). Newer homes typically include more bathrooms to match contemporary expectations for convenience and functionality, making bathroom count a reliable indicator of more recent construction.\nThe number of stories (stories) feature also contributed to the classification model, although with a lower importance score. This is still valuable because single-story homes were historically more common in earlier decades, whereas modern subdivisions often include two-story designs.\n\n\nShow the code\n# Decision Tree importance\nimportance_tree = pd.Series(clf_tree.feature_importances_, index=features.columns).sort_values()\nplt.figure(figsize=(8, 5))\nimportance_tree.plot(kind='barh', color='lightseagreen', edgecolor='black')\nplt.title('Decision Tree Feature Importance')\nplt.xlabel('Importance Score')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Random Forest importance\nimportance_rf = pd.Series(clf_rf.feature_importances_, index=features.columns).sort_values()\nplt.figure(figsize=(8, 5))\nimportance_rf.plot(kind='barh', color='salmon', edgecolor='black')\nplt.title('Random Forest Feature Importance')\nplt.xlabel('Importance Score')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese patterns are visualized in the accompanying feature importance chart below, which shows the ranked contribution of each feature to the model. The chart confirms that living area, number of bathrooms, and number of stories are the dominant variables. Overall, these variables align with real-world domain knowledge about housing design and construction patterns and justify the model’s predictions in a way that is explainable and transparent for stakeholders.\nFurther improvements could include adding neighborhood-level attributes or temporal price trends to enhance predictive power.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#questiontask-4",
    "href": "250_Projects/project4.html#questiontask-4",
    "title": "Client Report - Can You Predict That?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nI evaluated the classification models using three common metrics: accuracy, precision, and recall. Each provides a different perspective on model performance:\nAccuracy measures the overall proportion of correct predictions across both classes. The Random Forest achieved approximately 80% accuracy, slightly higher than the Decision Tree (78%) and Logistic Regression (79%). However, accuracy alone can be misleading if the classes are imbalanced, which is why we also examine precision and recall.\n\n\nShow the code\nprint(\"Decision Tree Evaluation:\")\nprint(classification_report(y_test, y_pred_tree))\nprint(confusion_matrix(y_test, y_pred_tree))\n\n\nDecision Tree Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.73      0.67      0.70      2569\n           1       0.81      0.85      0.83      4305\n\n    accuracy                           0.78      6874\n   macro avg       0.77      0.76      0.77      6874\nweighted avg       0.78      0.78      0.78      6874\n\n[[1714  855]\n [ 625 3680]]\n\n\nPrecision measures how many predicted positives were actually correct. For example, Random Forest had a precision of 0.84 for the positive (before1980) class, meaning when it predicts a home is pre-1980, it is correct 84% of the time. Precision is especially important if a false positive (wrongly classifying a newer home as old) has public health or safety consequences, such as unnecessary asbestos remediation.\n\n\nShow the code\nprint(\"\\nRandom Forest Evaluation:\")\nprint(classification_report(y_test, y_pred_rf))\nprint(confusion_matrix(y_test, y_pred_rf))\n\n\n\nRandom Forest Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.74      0.73      0.73      2569\n           1       0.84      0.85      0.84      4305\n\n    accuracy                           0.80      6874\n   macro avg       0.79      0.79      0.79      6874\nweighted avg       0.80      0.80      0.80      6874\n\n[[1867  702]\n [ 653 3652]]\n\n\nRecall measures how many true positives were captured among all actual positives. The Random Forest achieved a recall of 0.85 for the positive class, meaning it correctly identified 85% of homes that were truly built before 1980. High recall is crucial if you want to avoid missing any potentially hazardous homes.\n\n\nShow the code\nprint(\"\\nLogistic Regression Evaluation:\")\nprint(classification_report(y_test, y_pred_lr))\nprint(confusion_matrix(y_test, y_pred_lr))\n\n\n\nLogistic Regression Evaluation:\n              precision    recall  f1-score   support\n\n           0       0.76      0.62      0.69      2569\n           1       0.80      0.88      0.84      4305\n\n    accuracy                           0.79      6874\n   macro avg       0.78      0.75      0.76      6874\nweighted avg       0.78      0.79      0.78      6874\n\n[[1602  967]\n [ 496 3809]]\n\n\nAs a balanced measure, the f1-score combines precision and recall, showing the Random Forest at 0.84 for the positive class, which is a solid compromise between missing too many cases and misclassifying safe homes.\nInterpretation of confusion matrices shows most of the model errors were between these borderline homes built near 1980, which is expected. For example, the Random Forest confusion matrix shows 702 false positives (newer homes classified as old) and 653 false negatives (older homes classified as new). This tradeoff is acceptable depending on whether missing a hazardous home (false negative) is worse than sending a safe home for inspection (false positive).\nOverall, while no model reached the 90% accuracy target, the Random Forest achieved the best balance of precision, recall, and interpretability, making it the most practical choice for a production environment. With further feature engineering or additional data, its performance could be improved.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#stretch-questiontask-1",
    "href": "250_Projects/project4.html#stretch-questiontask-1",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nFor this stretch question, I tested three different algorithms to classify whether a home was built before 1980: Random Forest, Logistic Regression, and XGBoost. Each model was evaluated with a confusion matrix and either feature importance or coefficient values.\nAt first, all three models showed perfect or near-perfect accuracy, which seemed too good to be true. After checking, I realized the problem was that I had included yrbuilt as a feature, which is basically the answer to whether a house was built before 1980. Including it let the models “cheat” by memorizing the target, which is why the accuracy was 100%.\nRandom Forest’s feature importances and XGBoost’s results both confirmed this, because yrbuilt was by far the most dominant variable. Logistic Regression showed the same thing, with a huge coefficient on yrbuilt.\nIf yrbuilt is removed (which it should be, since you wouldn’t know it when predicting), Random Forest is still the strongest option. In earlier testing without yrbuilt, it achieved around 80% accuracy with a good balance of precision and recall. That makes it the most reliable recommendation for the client right now, with potential for improvement if more features or neighborhood data are added in the future.\n\n\nShow the code\n# Stretch Task 1 (corrected - no leakage)\nfrom xgboost import XGBClassifier\n\n# load from URL\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\njoined = pd.read_csv(url)\n\n# target\ny = joined[\"before1980\"]\n\n# drop leakage columns (yrbuilt and parcel)\nX = joined.drop(joined.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\nX = X.fillna(0)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\ndef print_model_results(model, X_test, y_test, feature_names=None):\n    preds = model.predict(X_test)\n    cm = confusion_matrix(y_test, preds)\n    print(f\"\\n{model.__class__.__name__} Confusion Matrix:\")\n    print(pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Pred 0\", \"Pred 1\"]))\n    print(classification_report(y_test, preds))\n    if hasattr(model, \"feature_importances_\"):\n        fi = pd.Series(model.feature_importances_, index=feature_names)\n        print(\"Feature Importances:\")\n        print(fi.sort_values(ascending=False))\n    elif hasattr(model, \"coef_\"):\n        coefs = pd.Series(model.coef_[0], index=feature_names)\n        print(\"Coefficients:\")\n        print(coefs.sort_values(ascending=False))\n\n# 1. Random Forest\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\nprint_model_results(rf, X_test, y_test, X.columns)\n\n# 2. Logistic Regression\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\nprint_model_results(lr, X_test, y_test, X.columns)\n\n# 3. XGBoost\nxgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nxgb.fit(X_train, y_train)\nprint_model_results(xgb, X_test, y_test, X.columns)\n\n\n\nRandomForestClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1557     162\nActual 1     167    2697\n              precision    recall  f1-score   support\n\n           0       0.90      0.91      0.90      1719\n           1       0.94      0.94      0.94      2864\n\n    accuracy                           0.93      4583\n   macro avg       0.92      0.92      0.92      4583\nweighted avg       0.93      0.93      0.93      4583\n\nFeature Importances:\nlivearea                            0.089314\narcstyle_ONE-STORY                  0.087884\nstories                             0.066375\nnumbaths                            0.061540\ntasp                                0.060614\nnetprice                            0.059995\nsprice                              0.056965\ngartype_Att                         0.053755\nbasement                            0.041581\nquality_C                           0.035654\nabstrprd                            0.035380\nquality_B                           0.030168\nnocars                              0.027651\narcstyle_TWO-STORY                  0.026571\nnumbdrm                             0.025587\nfinbsmnt                            0.025496\nsmonth                              0.025447\ngartype_Det                         0.022708\ncondition_AVG                       0.022509\ncondition_Good                      0.017316\ndeduct                              0.017142\nstatus_V                            0.014987\narcstyle_END UNIT                   0.012977\nstatus_I                            0.011710\nsyear                               0.011419\narcstyle_MIDDLE UNIT                0.009791\narcstyle_ONE AND HALF-STORY         0.009296\ngartype_None                        0.008714\nqualified_U                         0.006182\nqualified_Q                         0.006019\nquality_A                           0.003753\narcstyle_TRI-LEVEL WITH BASEMENT    0.003160\narcstyle_TRI-LEVEL                  0.002463\ntotunits                            0.002361\nquality_D                           0.001411\ncondition_VGood                     0.001095\narcstyle_BI-LEVEL                   0.000993\narcstyle_CONVERSIONS                0.000727\nquality_X                           0.000681\narcstyle_TWO AND HALF-STORY         0.000564\ngartype_Att/Det                     0.000551\ngartype_det/CP                      0.000542\narcstyle_SPLIT LEVEL                0.000428\narcstyle_THREE-STORY                0.000336\ncondition_Excel                     0.000100\ngartype_CP                          0.000073\ngartype_att/CP                      0.000017\ncondition_Fair                      0.000000\ndtype: float64\n\n\n\nLogisticRegression Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1316     403\nActual 1     287    2577\n              precision    recall  f1-score   support\n\n           0       0.82      0.77      0.79      1719\n           1       0.86      0.90      0.88      2864\n\n    accuracy                           0.85      4583\n   macro avg       0.84      0.83      0.84      4583\nweighted avg       0.85      0.85      0.85      4583\n\nCoefficients:\narcstyle_ONE-STORY                  0.814494\nnumbdrm                             0.736098\ngartype_Det                         0.654633\ncondition_Good                      0.589313\nquality_C                           0.528843\nstatus_I                            0.220676\narcstyle_ONE AND HALF-STORY         0.164160\ntotunits                            0.149902\nqualified_U                         0.114312\nsmonth                              0.099839\ncondition_VGood                     0.059827\narcstyle_CONVERSIONS                0.039922\nquality_D                           0.027522\narcstyle_TRI-LEVEL                  0.022280\narcstyle_TRI-LEVEL WITH BASEMENT    0.020910\ngartype_Att/Det                     0.020880\narcstyle_TWO AND HALF-STORY         0.016749\ngartype_det/CP                      0.015078\narcstyle_BI-LEVEL                   0.012208\ngartype_CP                          0.011826\narcstyle_THREE-STORY                0.004782\ngartype_att/CP                      0.002044\nsyear                               0.001461\nfinbsmnt                            0.000979\nbasement                            0.000817\ncondition_Excel                     0.000242\ntasp                                0.000003\nnetprice                            0.000002\ncondition_Fair                      0.000000\nsprice                             -0.000005\ndeduct                             -0.000007\nlivearea                           -0.000321\nabstrprd                           -0.000778\narcstyle_SPLIT LEVEL               -0.011953\nquality_X                          -0.014574\ngartype_None                       -0.023020\nquality_A                          -0.040403\nnocars                             -0.047272\nqualified_Q                        -0.114299\nstatus_V                           -0.220663\narcstyle_MIDDLE UNIT               -0.336900\narcstyle_TWO-STORY                 -0.350703\narcstyle_END UNIT                  -0.395938\nquality_B                          -0.501375\ncondition_AVG                      -0.649369\ngartype_Att                        -0.681428\nstories                            -0.732746\nnumbaths                           -1.140804\ndtype: float64\n\nXGBClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1569     150\nActual 1     161    2703\n              precision    recall  f1-score   support\n\n           0       0.91      0.91      0.91      1719\n           1       0.95      0.94      0.95      2864\n\n    accuracy                           0.93      4583\n   macro avg       0.93      0.93      0.93      4583\nweighted avg       0.93      0.93      0.93      4583\n\nFeature Importances:\narcstyle_ONE-STORY                  0.438540\nquality_C                           0.115159\ngartype_Att                         0.095272\ncondition_AVG                       0.040470\narcstyle_ONE AND HALF-STORY         0.037402\nstatus_I                            0.035846\nstories                             0.034622\nabstrprd                            0.018647\nnumbaths                            0.016600\ntotunits                            0.012407\nnocars                              0.011552\nnumbdrm                             0.009490\narcstyle_MIDDLE UNIT                0.008742\nbasement                            0.008642\nqualified_Q                         0.008504\ngartype_Det                         0.008193\nnetprice                            0.007239\ntasp                                0.007181\narcstyle_TRI-LEVEL                  0.006992\nlivearea                            0.006735\nfinbsmnt                            0.006542\narcstyle_END UNIT                   0.005841\nquality_D                           0.005253\nsprice                              0.005075\narcstyle_TWO AND HALF-STORY         0.004823\ndeduct                              0.003967\nquality_A                           0.003895\nquality_B                           0.003742\nsyear                               0.003225\narcstyle_BI-LEVEL                   0.003192\ncondition_Good                      0.003037\narcstyle_TWO-STORY                  0.002935\narcstyle_THREE-STORY                0.002904\nquality_X                           0.002877\ngartype_det/CP                      0.002451\ncondition_VGood                     0.002440\nsmonth                              0.002305\narcstyle_CONVERSIONS                0.002303\narcstyle_TRI-LEVEL WITH BASEMENT    0.002014\ngartype_Att/Det                     0.001411\narcstyle_SPLIT LEVEL                0.000970\ncondition_Excel                     0.000564\ngartype_CP                          0.000000\ncondition_Fair                      0.000000\ngartype_att/CP                      0.000000\ngartype_None                        0.000000\nqualified_U                         0.000000\nstatus_V                            0.000000\ndtype: float32",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#stretch-questiontask-2",
    "href": "250_Projects/project4.html#stretch-questiontask-2",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 2",
    "text": "STRETCH QUESTION|TASK 2\nAfter merging the neighborhood data with the dwellings data, I reran the same three algorithms: Random Forest, Logistic Regression, and XGBoost. All three models showed perfect or near-perfect accuracy again, with 100% classification rates, which is a strong indicator of data leakage. This happened because the yrbuilt column was still included as a feature after joining, which lets the models essentially memorize the answer.\nThe feature importances and coefficients confirmed this: yrbuilt was still the dominant driver in every model, overwhelming the effect of the new neighborhood variables. The added neighborhood features (like the nbhd_ variables) contributed almost nothing, showing extremely low or even zero importance scores.\nBecause of this, the results with the merged dataset do not actually change the recommended model. If we remove yrbuilt from the features, Random Forest would still likely perform best, similar to its ~80% accuracy from earlier runs. The neighborhood features might provide a small boost if the model is properly cleaned of leakage, but on their own they did not shift the model’s decision boundaries in a meaningful way.\nIn short, joining the neighborhood data did not meaningfully improve the models when yrbuilt was present, but could be helpful in the future if handled carefully and after removing data leakage.\n\n\nShow the code\n# Stretch Task 2 (corrected - no leakage)\n\n# get neighborhood data from URL:\nneigh_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\"\ndwell_url = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\nneigh = pd.read_csv(neigh_url)\ndwell = pd.read_csv(dwell_url)\n\n# merge\njoined2 = dwell.merge(neigh, on=\"parcel\", how=\"left\")\n\n# target\ny2 = joined2[\"before1980\"]\n\n# drop leakage columns\nX2 = joined2.drop(joined2.filter(regex=\"before1980|yrbuilt|parcel\").columns, axis=1)\nX2 = X2.fillna(0)\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(\n    X2, y2, test_size=0.2, stratify=y2, random_state=42\n)\n\n# 1. Random Forest\nrf2 = RandomForestClassifier(random_state=42)\nrf2.fit(X2_train, y2_train)\nprint_model_results(rf2, X2_test, y2_test, X2.columns)\n\n# 2. Logistic Regression\nlr2 = LogisticRegression(max_iter=1000)\nlr2.fit(X2_train, y2_train)\nprint_model_results(lr2, X2_test, y2_test, X2.columns)\n\n# 3. XGBoost\nxgb2 = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nxgb2.fit(X2_train, y2_train)\nprint_model_results(xgb2, X2_test, y2_test, X2.columns)\n\n\n\nRandomForestClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    2039      92\nActual 1      83    3379\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      2131\n           1       0.97      0.98      0.97      3462\n\n    accuracy                           0.97      5593\n   macro avg       0.97      0.97      0.97      5593\nweighted avg       0.97      0.97      0.97      5593\n\nFeature Importances:\nstories               6.516791e-02\narcstyle_ONE-STORY    6.516508e-02\nlivearea              6.253251e-02\nnumbaths              4.826818e-02\ngartype_Att           4.324101e-02\n                          ...     \nnbhd_214              3.969820e-08\nnbhd_659              3.945910e-08\nnbhd_653              1.889511e-08\nnbhd_648              1.610502e-08\ncondition_Fair        0.000000e+00\nLength: 321, dtype: float64\n\n\n\nLogisticRegression Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    1640     491\nActual 1     295    3167\n              precision    recall  f1-score   support\n\n           0       0.85      0.77      0.81      2131\n           1       0.87      0.91      0.89      3462\n\n    accuracy                           0.86      5593\n   macro avg       0.86      0.84      0.85      5593\nweighted avg       0.86      0.86      0.86      5593\n\nCoefficients:\narcstyle_ONE-STORY    0.845281\nnumbdrm               0.748910\ngartype_Det           0.720395\ncondition_Good        0.667724\nquality_C             0.515691\n                        ...   \nquality_B            -0.484006\ncondition_AVG        -0.727179\ngartype_Att          -0.750383\nstories              -0.792453\nnumbaths             -1.151043\nLength: 321, dtype: float64\n\n\n\nXGBClassifier Confusion Matrix:\n          Pred 0  Pred 1\nActual 0    2018     113\nActual 1      77    3385\n              precision    recall  f1-score   support\n\n           0       0.96      0.95      0.96      2131\n           1       0.97      0.98      0.97      3462\n\n    accuracy                           0.97      5593\n   macro avg       0.97      0.96      0.96      5593\nweighted avg       0.97      0.97      0.97      5593\n\nFeature Importances:\narcstyle_ONE-STORY    0.307648\ngartype_Att           0.083395\nquality_C             0.069829\nnbhd_101              0.027231\nstories               0.025006\n                        ...   \nnbhd_708              0.000000\nnbhd_709              0.000000\nnbhd_710              0.000000\nnbhd_711              0.000000\nnbhd_714              0.000000\nLength: 321, dtype: float32",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project4.html#stretch-questiontask-3",
    "href": "250_Projects/project4.html#stretch-questiontask-3",
    "title": "Client Report - Can You Predict That?",
    "section": "STRETCH QUESTION|TASK 3",
    "text": "STRETCH QUESTION|TASK 3\nFor this stretch question, I built a regression model to predict the year a house was built using a Random Forest Regressor. The model achieved a root mean squared error (RMSE) of about 11.6 years, meaning on average predictions were within roughly 12 years of the true build year. The median absolute error was lower, at around 3.9 years, which shows that half of the predictions were off by less than four years — a good sign that the model handles most houses reasonably well, with a few larger outliers.\nThe R² value was approximately 0.901, which means the model explained about 90% of the variance in the year built. Overall, this is a strong score for a regression problem with a complex target like construction year.\nWhile the random forest did a good job predicting year built, there is room to improve by adding more external features, such as neighborhood development data or historical zoning codes, to tighten the RMSE even further. Still, this model would provide a solid starting point for helping the client estimate missing build years when needed.\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Load your confirmed data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n\n# set target\ny_reg = df[\"yrbuilt\"]\n\n# drop target and identifier\nX_reg = df.drop(columns=[\"yrbuilt\", \"parcel\"])\nX_reg = X_reg.fillna(0)\n\n# split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\n# Random Forest Regressor\nrf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_reg.fit(X_train, y_train)\n\n# predict\ny_pred = rf_reg.predict(X_test)\n\n# evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))   # fixes the error\nr2 = r2_score(y_test, y_pred)\nmedae = median_absolute_error(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R²: {r2:.3f}\")\nprint(f\"Median Absolute Error: {medae:.2f}\")\n\n# residual plot\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals)\nplt.axhline(0, color=\"red\")\nplt.xlabel(\"Predicted Year Built\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot\")\nplt.show()\n\n# feature importances\npd.Series(rf_reg.feature_importances_, index=X_reg.columns).sort_values().plot(\n    kind=\"barh\", figsize=(10,12)\n)\nplt.title(\"Feature Importances for Year Built Regression\")\nplt.tight_layout()\nplt.show()\n\n\nRMSE: 11.60\nR²: 0.901\nMedian Absolute Error: 3.88",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "250_Projects/project3.html",
    "href": "250_Projects/project3.html",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Show the code\nimport pandas as pd\nimport numpy as np\nfrom lets_plot import *\n\nLetsPlot.setup_html(isolated_frame=True)\nShow the code\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\ndf = pd.read_json(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")\n\ndf.columns.to_list()\n\n\n['airport_code',\n 'airport_name',\n 'month',\n 'year',\n 'num_of_flights_total',\n 'num_of_delays_carrier',\n 'num_of_delays_late_aircraft',\n 'num_of_delays_nas',\n 'num_of_delays_security',\n 'num_of_delays_weather',\n 'num_of_delays_total',\n 'minutes_delayed_carrier',\n 'minutes_delayed_late_aircraft',\n 'minutes_delayed_nas',\n 'minutes_delayed_security',\n 'minutes_delayed_weather',\n 'minutes_delayed_total']",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#elevator-pitch",
    "href": "250_Projects/project3.html#elevator-pitch",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nThe analysis I did discovers that SLC airport had the highest proportion of delayed flights, February was the most punctual month to travel. Weather-related delays were more prevalent in late summer and winter, with SFO airport experiencing the greatest weather impact.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#questiontask-1",
    "href": "250_Projects/project3.html#questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.__\nBy replacing empty string and “999” with NaN. This standardizes the data and gets rid of placeholder values and empty strings. It also allows us to handle missing values and makes them easier to identify. The example below is a single row from the data set that has been cleaned and turned into JSON format.\n\n\nShow the code\n#This code replaces empty string and \"999\" with NaN\ndf_clean= df.replace(['', 999], np.nan)\n\n# Clean airport code formatting right after cleaning the data\ndf_clean['airport_code'] = df_clean['airport_code'].str.strip().str.upper()\n\nexample= df_clean.iloc[[10]].to_json(orient='records', indent = 2)\nprint(example)\n\n\n[\n  {\n    \"airport_code\":\"ORD\",\n    \"airport_name\":\"Chicago, IL: Chicago O'Hare International\",\n    \"month\":\"Febuary\",\n    \"year\":2005.0,\n    \"num_of_flights_total\":25665,\n    \"num_of_delays_carrier\":\"794\",\n    \"num_of_delays_late_aircraft\":1222.0,\n    \"num_of_delays_nas\":3132,\n    \"num_of_delays_security\":6,\n    \"num_of_delays_weather\":114,\n    \"num_of_delays_total\":5269,\n    \"minutes_delayed_carrier\":50136.0,\n    \"minutes_delayed_late_aircraft\":72868,\n    \"minutes_delayed_nas\":164398.0,\n    \"minutes_delayed_security\":208,\n    \"minutes_delayed_weather\":7256,\n    \"minutes_delayed_total\":294866\n  }\n]",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#questiontask-2",
    "href": "250_Projects/project3.html#questiontask-2",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Describe the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nThe proportion I used to identify the airport with worst delays is the proportion of delayed flights. This is crucial for identifying airports where delays happen often. A small airport would have fewere total minutes of delay but a very high proportion of late departures, which could still make it the “worst” from a passenger’s perspective. I also used the average delay time in hours so that we can determine how severe the delays are, on average, to get an accurate depiction of airports when delays occur. A hgigher proportion of delays may indicate frequency, average delay time indicaes the intensity.\n\n\nShow the code\n## QUESTION|TASK 2\n\n# Create delay indicator and helper column for average delay\ndf_clean['is_delayed'] = df_clean['minutes_delayed_total'].fillna(0) &gt; 0\ndf_clean['delay_minutes_filled'] = df_clean['minutes_delayed_total'].fillna(0)\n\n# Summary by airport\nsummary = (\n    df_clean.groupby('airport_code')\n    .agg(\n        total_flights=('num_of_flights_total', 'sum'),\n        delayed_flights=('is_delayed', 'sum'),\n        avg_delay_hr=('delay_minutes_filled', lambda x: x.mean() / 60)\n    )\n    .reset_index()\n)\n\nsummary['prop_delayed'] = summary['delayed_flights'] / summary['total_flights']\nsummary = summary.sort_values(by='prop_delayed', ascending=False)\n\n# Optional: Display the summary table\nprint(summary.head())\n\n\n  airport_code  total_flights  delayed_flights  avg_delay_hr  prop_delayed\n2          IAD         851571              132   1298.418939      0.000155\n4          SAN         917862              132   1044.980808      0.000144\n6          SLC        1403384              132   1278.203409      0.000094\n5          SFO        1630945              132   3352.334975      0.000081\n1          DEN        2513974              132   3178.457197      0.000053\n\n\nBased on the findings, the highest proportion of delayed flights was IAD (Washington Dulles) with the worst delays, even though its a smaller airport. Passengers that fly through IAD are more likely to experience delays. If we take into account the average delay duratio, ORD (Chicago O’Hare) would be the worst airport. Suggesting that even though delays are less likely and less frequent than IAD, they are significantly longer when they do happen. IAD is the worst in frequency and ORD is the worst in severity.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#questiontask-3",
    "href": "250_Projects/project3.html#questiontask-3",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nTo identify the best month to fly, I analyzed average delay per flight by month. I chose this metric because it accounts for both the frequency and intensity of delays, rather than just whether any delay occurred. By calculating the total minutes of delay divided by the total number of flights for each airport-month, then averaging these across all airports, we get a fair comparison of punctuality month by month.\n\n\nShow the code\n# Filter and clean the data\ndf_month = df_clean.copy()\n\n# Drop rows missing month or flights\ndf_month = df_month.dropna(subset=['month', 'num_of_flights_total'])\n\n# Map month names to numbers\nmonth_name_to_num = {\n    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n    'September': 9, 'October': 10, 'November': 11, 'December': 12\n}\ndf_month['month_num'] = df_month['month'].map(month_name_to_num)\n\n# Fix missing delay values so months like February aren’t dropped\ndf_month['minutes_delayed_total'] = pd.to_numeric(df_month['minutes_delayed_total'], errors='coerce').fillna(0)\n\n# Drop if num_of_flights_total is 0\ndf_month = df_month[df_month['num_of_flights_total'] &gt; 0]\n\n# Calculate average delay per flight\ndf_month['avg_delay_per_flight_hr'] = df_month['minutes_delayed_total'] / df_month['num_of_flights_total'] / 60\n\n# Group by month\nmonth_summary = (\n    df_month.groupby(['month', 'month_num'], as_index=False)\n    .agg(avg_delay_hr=('avg_delay_per_flight_hr', 'mean'))\n    .sort_values('month_num')\n)\n\n# Check which months are actually present\nprint(month_summary['month'])\n\n# Plot\nggplot(month_summary, aes(x='month', y='avg_delay_hr')) + \\\n    geom_bar(stat='identity', fill='#87CEEB') + \\\n    labs(title='Average Delay per Flight by Month (Hours)',\n         x='Month', y='Average Delay (Hours)') + \\\n    theme(axis_text_x=element_text(angle=45, hjust=1))\n\n\n3       January\n6         March\n0         April\n7           May\n5          June\n4          July\n1        August\n10    September\n9       October\n8      November\n2      December\nName: month, dtype: object\n\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\n\n\nShow the code\nprint(df_clean[df_clean['month'] == 'February'])\n\n\nEmpty DataFrame\nColumns: [airport_code, airport_name, month, year, num_of_flights_total, num_of_delays_carrier, num_of_delays_late_aircraft, num_of_delays_nas, num_of_delays_security, num_of_delays_weather, num_of_delays_total, minutes_delayed_carrier, minutes_delayed_late_aircraft, minutes_delayed_nas, minutes_delayed_security, minutes_delayed_weather, minutes_delayed_total, is_delayed, delay_minutes_filled]\nIndex: []\n\n\nThe chart displays the average delay per flight by month. Based on the data, September has the lowest average delays, making it the best month to travel if your goal is to avoid flight delays.\nThis metric reflects both the frequency and intensity of delays by dividing total delay minutes by total flights per month.\nNote: February does not appear in the dataset, likely due to missing or incomplete data for that month.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#questiontask-4",
    "href": "250_Projects/project3.html#questiontask-4",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories.\nThis section combines delays from three sources: 100% of flights delayed due to severe weather, 30% of delays from late-arriving aircraft, and a month-based proportion of NAS delays (40% from April to August, 65% otherwise). This provides a more complete picture of how both severe and mild weather conditions contribute to flight delays. The data shows that ORD (Chicago O’Hare) experiences the highest total number of weather-related delays (4,502 flights), suggesting it is especially vulnerable to both direct and indirect weather impacts. ATL (Atlanta) also has a substantial number of delays (~3,769), driven largely by NAS-related disruptions. In contrast, airports like IAD (Washington Dulles) and SAN (San Diego) show lower weather-related delay totals, indicating they are either less exposed to disruptive weather or more resilient in handling its effects. This approach offers a more realistic view of weather’s full influence on airport performance.\n\n\nShow the code\n# Fill missing values for the relevant columns\ndf_clean['num_of_delays_late_aircraft'] = pd.to_numeric(df_clean['num_of_delays_late_aircraft'], errors='coerce')\ndf_clean['num_of_delays_weather'] = pd.to_numeric(df_clean['num_of_delays_weather'], errors='coerce').fillna(0)\ndf_clean['num_of_delays_nas'] = pd.to_numeric(df_clean['num_of_delays_nas'], errors='coerce').fillna(0)\n\n# Replace negative or invalid late_aircraft values with NaN first\ndf_clean.loc[df_clean['num_of_delays_late_aircraft'] &lt; 0, 'num_of_delays_late_aircraft'] = np.nan\n\n# Fill missing late_aircraft with mean\nlate_mean = df_clean['num_of_delays_late_aircraft'].mean()\ndf_clean['num_of_delays_late_aircraft'] = df_clean['num_of_delays_late_aircraft'].fillna(late_mean)\n\n\n\n\nShow the code\n# Map month names to numbers\nmonth_to_num = {\n    'January': 1, 'February': 2, 'March': 3, 'April': 4,\n    'May': 5, 'June': 6, 'July': 7, 'August': 8,\n    'September': 9, 'October': 10, 'November': 11, 'December': 12\n}\ndf_clean['month_num'] = df_clean['month'].map(month_to_num)\n\n# Define the NAS weather delay proportion rule\ndef nas_weather_proportion(month_num):\n    if pd.isna(month_num):\n        return 0\n    return 0.4 if 4 &lt;= month_num &lt;= 8 else 0.65\n\n\n\n\nShow the code\n# Calculate combined weather delay using all rules\ndf_clean['weather_delay_all'] = (\n    df_clean['num_of_delays_weather'] +\n    0.3 * df_clean['num_of_delays_late_aircraft'] +\n    df_clean['num_of_delays_nas'] * df_clean['month_num'].apply(nas_weather_proportion)\n)\n\n# Display first 5 rows of relevant columns\ndf_clean[['airport_code', 'month', 'num_of_delays_weather',\n          'num_of_delays_late_aircraft', 'num_of_delays_nas',\n          'weather_delay_all']].rename(columns={\n    'airport_code': 'airport',\n    'num_of_delays_weather': 'delays_weather',\n    'num_of_delays_late_aircraft': 'late_aircraft',\n    'num_of_delays_nas': 'delays_nas'\n}).head()\n\n\n\n\n\n\n\n\n\nairport\nmonth\ndelays_weather\nlate_aircraft\ndelays_nas\nweather_delay_all\n\n\n\n\n0\nATL\nJanuary\n448\n1109.228766\n4598\n3769.46863\n\n\n1\nDEN\nJanuary\n233\n928.000000\n935\n1119.15000\n\n\n2\nIAD\nJanuary\n61\n1058.000000\n895\n960.15000\n\n\n3\nORD\nJanuary\n306\n2255.000000\n5415\n4502.25000\n\n\n4\nSAN\nJanuary\n56\n680.000000\n638\n674.70000",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#questiontask-5",
    "href": "250_Projects/project3.html#questiontask-5",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\nThis chart shows the proportion of total flights at each airport that were delayed by weather, accounting for both severe and mild delays. Airports like SFO and ORD show the highest proportions of weather-related delays. In contrast, airports such as SLC and SAN have relatively fewer weather-related delays, suggesting more stable flying conditions.\n\n\nShow the code\n# Fix potential missing data\ndf_clean['weather_delay_all'] = df_clean['weather_delay_all'].fillna(0)\ndf_clean['num_of_flights_total'] = df_clean['num_of_flights_total'].fillna(0)\n\n# Summarize again\nweather_summary = (\n    df_clean.groupby('airport_code')\n    .agg(\n        total_flights=('num_of_flights_total', 'sum'),\n        total_weather_delays=('weather_delay_all', 'sum')\n    )\n    .assign(prop_weather_delay=lambda d: d['total_weather_delays'] / d['total_flights'])\n    .query('total_flights &gt; 0')\n    .reset_index()\n)\n\n# Sort for plotting\nweather_summary_sorted = weather_summary.sort_values('prop_weather_delay', ascending=False)\n\n# Plot\nggplot(weather_summary_sorted, aes(x='airport_code', y='prop_weather_delay')) + \\\n    geom_bar(stat='identity', fill='#FF7F7F') + \\\n    labs(title='Proportion of Total Flights Delayed by Weather',\n         x='Airport', y='Proportion of Flights') + \\\n    theme(axis_text_x=element_text(size=10, angle=45, hjust=1))",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "250_Projects/project3.html#stretch-questiontask-1",
    "href": "250_Projects/project3.html#stretch-questiontask-1",
    "title": "Client Report - Late Flights & Missing Data (JSON)",
    "section": "STRETCH QUESTION|TASK 1",
    "text": "STRETCH QUESTION|TASK 1\nWhich delay is the worst delay?\nThe analysis shows that weather-related delays are the most significant, affecting approximately 6.5% of all flights. These delays include both severe and mild weather impacts, offering a more comprehensive view of real-world disruptions. Carrier delays, which typically stem from airline operational issues, are also notable at 3.8%. In contrast, security delays are extremely rare, impacting less than 0.1% of total flights. This comparison highlights that weather is the dominant factor in flight disruptions, both in terms of frequency and impact.\n\n\nShow the code\n## STRETCH QUESTION|TASK 1\ncarrier = pd.to_numeric(df_clean['num_of_delays_carrier'], errors='coerce').fillna(0).sum()\nsecurity = pd.to_numeric(df_clean['num_of_delays_security'], errors='coerce').fillna(0).sum()\nweather = pd.to_numeric(df_clean['weather_delay_all'], errors='coerce').fillna(0).sum()\n\n# Total flights\ntotal_flights = pd.to_numeric(df_clean['num_of_flights_total'], errors='coerce').fillna(0).sum()\n\n# Build the DataFrame\ntotals = pd.DataFrame({\n    'delay_type': ['Carrier', 'Security', 'Weather'],\n    'total_delays': [carrier, security, weather]\n})\n\n# Ensure all are numeric\ntotals['total_delays'] = pd.to_numeric(totals['total_delays'], errors='coerce')\n\n# Calculate proportions\ntotals['prop_delayed'] = totals['total_delays'] / total_flights\n\n# Sort\ntotals = totals.sort_values('prop_delayed', ascending=False)\n\n# Print safely\nprint(totals.to_string(index=False))\n\n\n# Optional plot\nggplot(totals, aes(x='delay_type', y='prop_delayed')) + \\\n    geom_bar(stat='identity', fill='#FF7F7F') + \\\n    labs(title='Proportion of Total Flights Delayed by Category',\n         x='Delay Type', y='Proportion of Flights') + \\\n    theme(axis_text_x=element_text(size=12))\n\n\ndelay_type  total_delays  prop_delayed\n   Weather  1.002633e+06      0.065338\n   Carrier  5.882920e+05      0.038337\n  Security  5.006000e+03      0.000326\n\n\n\n   \n       \n       \n       \n   \n   \n          \n   \n   \n\n\n\nThe bar chart clearly visualizes: Weather delays are the most common, affecting ~6.5% of all flights Carrier delays are next, affecting ~3.8% Security delays are extremely rare, under 0.1%",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Templates/DS250_Template.html",
    "href": "Templates/DS250_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "THIS .qmd IS INSTRUCTIONAL AND SHOULD NOT BE USED TO WRITE YOUR REPORTS (EXCEPTION - PROJECT 0). THERE IS ANOTHER TEMPLATE FILE FOR THAT. YOU WILL NEED TO PREVIEW THE REPORT TO PRODUCE A .html FILE. YOU WILL SUBMIT THE .html FILE ON CANVAS."
  },
  {
    "objectID": "Templates/DS250_Template.html#elevator-pitch",
    "href": "Templates/DS250_Template.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nA SHORT (2-3 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS. (Note: this is not a summary of the project, but a summary of the results.)\nA Client has requested this analysis and this is your one shot of what you would say to your boss in a 2 min elevator ride before he takes your report and hands it to the client.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\n# Note: using the URL is the easiest way for the data to still work in GitHub\n# You may download the file and reference it by name but only if you save it in the same folder as your .qmd file\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4python4ds/master/data-raw/mpg/mpg.csv'\ndf = pd.read_csv(url)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-1",
    "href": "Templates/DS250_Template.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 1",
    "text": "Question|Task 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\nAdd details here to answer the question but NOT like an assignment Q&A. You need to write your answers as a consulting solution report. A Client needs to understand the answer, but also needs to understand the decisions that went into the answer (when applicable).\ninclude figures in chunks and discuss your findings in the figure.\n\nYOU SHOULD HAVE QUALITY WRITING THAT DESCRIBES YOUR CHARTS AND TABLES.\nWE HIGHLY RECOMMEND GRAMMARLY TO FIX YOUR SPELLING AND GRAMMAR. WRITING TAKES TIME TO BE CLEAR. SPEND THE TIME TO PRACITCE.\nYOU SHOULD HAVE QUALITY COMMENTS THAT DESCRIBES YOUR CODES. OFTEN CODEERS WORK IN TEAMS AND YOU NEED TO HAVE QUALTIY COMMENTS FOR YOUR TEAM AND YOURSELF. YOU MAY NEED TO REVISIT CODE YOU WROTE OVER A YEAR AGO, AND IF YOU DONT COMMENT IT NOW YOU WONT REMEMBER WHY YOU DID WHAT YOU DID.\n\n\n\nRead and format data\n# Include and execute your code here"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-2",
    "href": "Templates/DS250_Template.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 2",
    "text": "Question|Task 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\n\nplot example\n# Include and execute your code here\n\n(\n  ggplot(df.head(500), aes(x='displ', y='hwy')) + geom_point()\n)\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n            \n            \n            \n              \n                3\n              \n            \n          \n          \n            \n            \n            \n              \n                4\n              \n            \n          \n          \n            \n            \n            \n              \n                5\n              \n            \n          \n          \n            \n            \n            \n              \n                6\n              \n            \n          \n          \n            \n            \n            \n              \n                7\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                15\n              \n            \n          \n          \n            \n              \n                20\n              \n            \n          \n          \n            \n              \n                25\n              \n            \n          \n          \n            \n              \n                30\n              \n            \n          \n          \n            \n              \n                35\n              \n            \n          \n          \n            \n              \n                40\n              \n            \n          \n          \n            \n              \n                45\n              \n            \n          \n        \n      \n    \n    \n      \n        hwy\n      \n    \n    \n      \n        displ\n      \n    \n    \n    \n  \n  \n  \n\nMy useless chart"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-3",
    "href": "Templates/DS250_Template.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 3",
    "text": "Question|Task 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\n\nPROVIDE TABLES THAT HELP ADDRESS THE QUESTIONS AND TASKS (IF APPLICABLE).\n\n\n\ntable example\n# Include and execute your code here\nmydat = (df.head(1000)\n    .groupby('manufacturer')\n    .sum()\n    .reset_index()\n    .tail(10)\n    .filter([\"manufacturer\",\"displ\",\"cty\", \"hwy\"])\n)\n\ndisplay(mydat)\n\n\n\n\n\n\ntable example\n\n\n\nmanufacturer\ndispl\ncty\nhwy\n\n\n\n\n5\nhyundai\n34.0\n261\n376\n\n\n6\njeep\n36.6\n108\n141\n\n\n7\nland rover\n17.2\n46\n66\n\n\n8\nlincoln\n16.2\n34\n51\n\n\n9\nmercury\n17.6\n53\n72\n\n\n10\nnissan\n42.5\n235\n320\n\n\n11\npontiac\n19.8\n85\n132\n\n\n12\nsubaru\n34.4\n270\n358\n\n\n13\ntoyota\n100.4\n630\n847\n\n\n14\nvolkswagen\n60.9\n565\n789"
  },
  {
    "objectID": "Templates/DS250_Template.html#questiontask-4",
    "href": "Templates/DS250_Template.html#questiontask-4",
    "title": "Client Report - [Insert Project Title]",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\n\nPROVIDE TABLES THAT HELP ADDRESS THE QUESTIONS AND TASKS (IF APPLICABLE).\n\n\n\ntable example\n# Include and execute your code here\n\n\nNote: Non executing Python Snippets include (3) ``` followed by (3) more ```, each on their own line. These are not single quotes, they are the key left of the number 1 key on the keyboard. The top row can include the language of code that is pasted inbetween the ``` marks.\nNote: These also work in Slack and it is expected they are used for any code shared in that app. No screen shots allowed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  }
]